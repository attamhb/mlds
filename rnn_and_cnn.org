
* Recurrent Neurons and Layers
A recurrent neural
network looks very much like a feedforward neural network, except it also
has connections pointing backward. 

For a simplest possible
RNN, composed of one neuron receiving inputs, producing an output, and
sending that output back to itself, at each
time step t (frame), this recurrent neuron receives the inputs
x(t) as well as its own output from the previous time step, y(t-1). 

Since there
is no previous output at the first time step, it is generally set to 0. 
*Unrolling The Network Through Time*


At each time step t,
every neuron receives both the input vector x(t) and the output vector from
the previous time step y(t-1). 
Each recurrent neuron has two sets of weights: 
w_x for the inputs x(t) 
w_y for the outputs of the previous time step y(t-1) 

y_(t) = ϕ(W^T_x x(t) + W_y^T y(t-1) + b)

Y_(t) = ϕ(X_(t) W_x + Y_(t−1) W_y + b)
= ϕ([X_(t) Y_(t−1)]W+b) with W =[W_x W_y]^T

- Y_(t) is an m × n_neurons matrix containing the layer's outputs at time
  step t for each instance in the mini-batch (m is the number of
  instances in the mini-batch and n_neurons is the number of neurons).

- X_(t) is an m × n_inputs matrix containing the inputs for all instances
  (n_inputs is the number of input features).

- W_x is an n_inputs × n_neurons matrix containing the connection
  weights for the inputs of the current time step.

- W_y is an n_neurons × n_neurons matrix containing the connection
  weights for the outputs of the previous time step. 

- b is a vector of size n_neurons containing each neuron's bias term.
- The weight matrices W_x and W_y are often concatenated vertically
  into a single weight matrix W of shape 
   (n_inputs + n_neurons) × n_neurons .
- [X_(t) Y_(t–1)] represents the horizontal concatenation
   of the matrices X_(t) and Y_(t-1).

- Y_(t) is a function of X_(t) and Y_(t-1), which is a function of X_(t–1)
  and Y_(t–2) , and so on. 

- This makes Y_(t) a function of all the inputs since time t = 0.

- At the first time step, t = 0, there are no previous outputs, so they are
  typically assumed to be all zeros.

* Memory Cells
The output of a recurrent neuron at time step t is a function of all the inputs
from previous time steps, it has a form of memory. A
part of a neural network that preserves some state across time steps is
called a memory cell. 

A single recurrent neuron, or a layer of recurrent neurons, is a very basic
cell, capable of learning only short patterns. A cell's state at time step t,
denoted h_(t), is a function of some inputs at that time step and its state at
the previous time step: h_(t) = f(h_(t–1), x_(t)).

Its output at time step t, denoted y_(t), is also a function of the previous
state and the current inputs. 

*  Input and Output Sequences

*Sequence-to-Sequence Network*
An RNN can simultaneously take a sequence of inputs and produce a
sequence of outputs. This type of
sequence-to-sequence network is useful for predicting time series. 

*Sequence-to-Vector Network*
In such networks, we feed the network a sequence of inputs and ignore all outputs except for the last one. 

*Vector-to-Sequence Network*
In such networks we feed the network the same input vector over and
over again at each time step and let it output a sequence. This is a
vector-to-sequence network. 


*Encoder–Decoder*
If we have a sequence-to-vector network, called an encoder,
followed by a vector-to-sequence network, called a decoder. This two-step
model, called an Encoder–Decoder. 

* Training RNNs
- To train an RNN, the unroll it through time and then simply use
  regular backpropagation. This strategy is called backpropagation through 
  time (BPTT). 

- There is a first forward pass through the unrolled network. 

- Then the output sequence is evaluated using a cost function 
  C(Y_(0), Y_(1), …Y_(T)).
  
- The gradients of that cost function are then propagated backward through the
  unrolled network. 

- Finally the model parameters are updated using the gradients computed during BPTT. 

- since the same parameters W and b are used at each time step,
  backpropagation will do the right thing and sum over all time steps.

* Forecasting a Time Series
- Time series  
-- Univariate time series
-- Multivariate time series. 

*Forcasting:* To predict future values.
*Imputation:* To fill in the blanks or to predict missing values from the
 past

 
For simplicity, we are using a time series generated by the
#+begin_src python :result outputs
def generate_time_series(batch_size, n_steps):
    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)
    time = np.linspace(0, 1, n_steps)
    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10)) 
    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) 
    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)
    return series[..., np.newaxis].astype(np.float32)

n_steps = 50
series = generate_time_series(10000, n_steps + 1)
X_train, y_train = series[:7000, :n_steps], series[:7000, -1]
X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]
X_test, y_test = series[9000:, :n_steps], series[9000:, -1]

#+end_src
The function creates as many time series returning a NumPy array of shape
[batch size, time steps, 1], where each series is the sum of two sine waves of
fixed amplitudes but random frequencies and phases, plus a bit of noise.

* Baseline Metrics
The simplest approach is to predict the last value in each series. This is
called naive forecasting, and it is sometimes surprisingly difficult to
outperform. In this case, it gives us a mean squared error of about 0.020:
#+begin_src python :result outputs
y_pred = X_valid[:, -1]
np.mean(keras.losses.mean_squared_error(y_valid, y_pred))
0.020211367
#+end_src
Another simple approach is to use a fully connected network. Since it
expects a flat list of features for each input, we need to add a Flatten
layer. Let's just use a simple Linear Regression model so that each
prediction will be a linear combination of the values in the time series:

#+begin_src python :result outputs
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[50, 1]),
    keras.layers.Dense(1)])
#+end_src
If we compile this model using the MSE loss and the default Adam
optimizer, then fit it on the training set for 20 epochs and evaluate it on
the validation set, we get an MSE of about 0.004. That's much better than
the naive approach!

* Implementing a Simple RNN
Let's see if we can beat that with a simple RNN:
#+begin_src python :result outputs
model = keras.models.Sequential([
keras.layers.SimpleRNN(1, input_shape=[None, 1])
])
#+end_src

It just contains a single layer, with a single neuron. We do not need to specify
the length of the input sequences, since a recurrent neural network can process
any number of time steps. By default, the SimpleRNN layer uses the hyperbolic
tangent activation function.

the initial state h(init) is set
to 0, and it is passed to a single recurrent neuron, 
along with the value of the first time step, x(0). 
The neuron computes a weighted sum of these values and
applies the hyperbolic tangent activation function to the result, and this gives
the first output, y_0. 
In a simple RNN, this output is also the new state h0. 
This new state is passed to the same recurrent neuron along with the next
input value, x(1), and the process is repeated until the last time step. 

Then the layer just outputs the last value, y_49. All of this is performed
simultaneously for every time series.

If you compile, fit, and evaluate this model, you will find that its MSE reaches only 0.014, so
it is better than the naive approach but it does not beat a simple linear
model. 
Note that for each neuron, a linear model has one parameter per
input and per time step, plus a bias term. In contrast, for each recurrent neuron
in a simple RNN, there is just one parameter per input and per hidden state
dimension, plus a bias term. 

In this simple RNN, that's a total of just three
parameters.

* Deep RNNs
Implementing a deep RNN with tf.keras is quite simple: just stack
recurrent layers. In this example, we use three SimpleRNN layers:
#+begin_src python :result outputs
model = keras.models.Sequential([
keras.layers.SimpleRNN(
          20, 
          return_sequences=True, input_shape=[None, 1]),
          keras.layers.SimpleRNN(20, return_sequences=True),
          keras.layers.SimpleRNN(1)
])
#+end_src
Note that the last layer is not ideal: 
it must have a single unit because we want to forecast a univariate time series,
and this means we must have a single output value per time step. 
However, having a single unit means
that the hidden state is just a single number. 

That's really not much, and
it's probably not that useful; presumably, the RNN will mostly use the
hidden states of the other recurrent layers to carry over all the information
it needs from time step to time step, and it will not use the final layer's
hidden state very much. 

Moreover, since a SimpleRNN layer uses the tanh activation function by default,
the predicted values must lie within the range –1 to 1. But what if you want to
use another activation function? For both these reasons, it might be preferable
to replace the output layer with a Dense layer: it would run slightly faster,
the accuracy would be roughly the same, and it would allow us to choose any
output activation function we want. If you make this change, also make sure to
remove

#+begin_src pyhton :result outputs
return_sequences=True from the second (now last) recurrent layer:
model = keras.models.Sequential([
keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),
keras.layers.SimpleRNN(20),
keras.layers.Dense(1)
])
#+end_src
This model converges faster and performs just as well. 
and you can change the output activation function if you wanted.
* Forecasting Several Time Steps Ahead
To predict the next 10 values, the first option is to use the model we already
trained, make it predict the next value, then add that value to the inputs,
and use the model again to predict the following value, and so on

#+begin_src python :result outputs
series = generate_time_series(1, n_steps + 10)
X_new, Y_new = series[:, :n_steps], series[:, n_steps:]
X = X_new
for step_ahead in range(10):
    y_pred_one = model.predict(X[:, step_ahead:])[:, np.newaxis, :]
    X = np.concatenate([X, y_pred_one], axis=1)
    Y_pred = X[:, n_steps:]
#+end_src
the prediction for the next step will usually be more
accurate than the predictions for later time steps, 
since the errors might
accumulate. 

This is much higher than the previous models, 
but it’s also a much harder task, so the
comparison doesn’t mean much. 

The naive approach is terrible,
but the linear model is much better than
using our RNN to forecast the future one step at a time, and also much
faster to train and run. 

Still, if you only want to forecast a few time steps
ahead, on more complex tasks, this approach may work well.

The second option is to train an RNN to predict all 10 next values at once.
We can still use a sequence-to-vector model, but it will output 10 values
instead of 1. 

#+begin_src python :result outputs
series = generate_time_series(10000, n_steps + 10)
X_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0]
X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]
X_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0]
#Now we just need the output layer to have 10 units instead of 1:
model = keras.models.Sequential([
keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None,
1]),
keras.layers.SimpleRNN(20),
keras.layers.Dense(10)
])
#After training , you can predict the next 10 values at once very easily:
Y_pred = model.predict(X_new)
#+end_src

This model works nicely and is much better than the linear model. 
To do better, we can train it to forecast the next 10 values at
each and every time step. 

The advantage of this technique is that the loss will contain a term for the
output of the RNN at each and every time step. This means there will be many
more error gradients flowing through the model, and they won't have to flow only
through time; they will also flow from the output of each time step.

This will both stabilize and speed up training.

At time step 0 the model will output a vector containing the
forecasts for time steps 1 to 10, 

Then at time step 1 the model will forecast time steps 2 to 11, and so on. 

#+begin_src python :result outputs
Y = np.empty((10000, n_steps, 10)) # each target is a sequence of 10D vectors
for step_ahead in range(1, 10 + 1):
    Y[:, :, step_ahead - 1] = series[:, step_ahead:step_ahead + n_steps, 0]
    Y_train = Y[:7000]
    Y_valid = Y[7000:9000]
    Y_test = Y[9000:]NOTE
#+end_src

It may be surprising that the targets will contain values that appear in the
inputs. At each time step, the model only knows about past time steps,
so it cannot look ahead. It is said to be a causal model.

To turn the model into a sequence-to-sequence model, we must set
return_sequences=True in all recurrent layers, and
we must apply the output Dense layer at every time step. 

#+begin_src python :result outputs
model = keras.models.Sequential([
          keras.layers.SimpleRNN(
          20, 
          return_sequences=True, 
          input_shape=[None, 1]),
          keras.layers.SimpleRNN(20, return_sequences=True),
          keras.layers.TimeDistributed(keras.layers.Dense(10))
])
#+end_src

The Dense layer actually supports sequences as inputs. We could
replace the last layer with just Dense(10). 

#+begin_src python :result outputs
def last_time_step_mse(Y_true, Y_pred):
    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])
optimizer = keras.optimizers.Adam(lr=0.01)
model.compile(loss="mse", optimizer=optimizer, metrics= [last_time_step_mse])
#+end_src
* Handling Long Sequences
To train an RNN on long sequences, we must run it over many time steps,
making the unrolled RNN a very deep network. 

Just like any deep neural network it may suffer from the unstable gradients
problem it may take forever to train, or training may be unstable.

Moreover, when an RNN processes a long sequence, it will gradually
forget the first inputs in the sequence. 
* Fighting the Unstable Gradients Problem
Nonsaturating activation functions they may actually lead the RNN to be even
more unstable during training. Suppose Gradient Descent updates the weights in a
way that increases the outputs slightly at the first time step. Because the same
weights are used at every time step, the outputs at the second time step may
also be slightly increased, and those at the third, and so on until the outputs
explode—and a nonsaturating activation function does not prevent that.

You can reduce this risk by using a smaller learning rate, but you can
lso simply use a saturating activation function like the hyperbolic tangent. 

Moreover, Batch Normalization cannot be used as efficiently with RNNs
as with deep feedforward nets. In fact, you cannot use it between time
steps, only between recurrent layers. 

To be more precise, it is technically
possible to add a BN layer to a memory cell so that
it will be applied at each time step. However, the same BNlayer will be used at each time step, with the same parameters, regardless
of the actual scale and offset of the inputs and hidden state. 

In practice, this does not yield good results it is found that BN was slightly
beneficial only when it was applied to the inputs, not to the hidden states. In
Keras this can be done simply by adding a BatchNormalization layer before each
recurrent layer, but don’t expect too much from it.

Layer Normalization often works better with RNNs: it is very similar to Batch
Normalization, but instead of normalizing across the batch dimension, it
normalizes across the features dimension.

One advantage is that it can compute the required statistics on
the fly, at each time step, independently for each instance. 

This also means that it behaves the same way during training and testing, and it does not need to use exponential moving averages to estimate
the feature statistics across all instances in the training set. 

Like BN, Layer Normalization learns a scale and an offset parameter for each
input. In an RNN, it is typically used right after the linear combination of the
inputs and the hidden states. 

Let's use tf.keras to implement Layer Normalization within a simple
memory cell. 

For this, we need to define a custom memory cell. 

#+begin_src python :result outputs
class LNSimpleRNNCell(keras.layers.Layer):

    def __init__(self, units, activation="tanh", **kwargs):
        super().__init__(**kwargs)
        self.state_size = units
        self.output_size = units
        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units,
        activation=None)
        self.layer_norm = keras.layers.LayerNormalization()
        self.activation = keras.activations.get(activation)

    def call(self, inputs, states):
        outputs, new_states = self.simple_rnn_cell(inputs, states)
        norm_outputs = self.activation(self.layer_norm(outputs))
        return norm_outputs, [norm_outputs]
model = keras.models.Sequential([
keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True,
input_shape=[None, 1]),
keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),
keras.layers.TimeDistributed(keras.layers.Dense(10))
])
#+end_src
* Tackling the Short-Term Memory Problem
Due to the transformations that the data goes through when traversing an
RNN, some information is lost at each time step. After a while, the RNN's
state contains virtually no trace of the first inputs. 
** LSTM cells
If you consider the LSTM cell as a black box, it can
be used very much like a basic cell, except it will perform much better;
training will converge faster, and it will detect long-term dependencies in
the data. In Keras, you can simply use the LSTM layer instead of the
SimpleRNN layer:
#+begin_src python :result outputs
model = keras.models.Sequential([
keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),
keras.layers.LSTM(20, return_sequences=True),
keras.layers.TimeDistributed(keras.layers.Dense(10))
])
# Alternatively, you could use the general-purpose keras.layers.RNN layer,
# giving it an LSTMCell as an argument:
model = keras.models.Sequential([
keras.layers.RNN(keras.layers.LSTMCell(20), return_sequences=True,
input_shape=[None, 1]),
keras.layers.RNN(keras.layers.LSTMCell(20), return_sequences=True),
keras.layers.TimeDistributed(keras.layers.Dense(10))
])
#+end_src

The LSTM layer uses an optimized implementation when running
on a GPU, so in general it is preferable to use it.

The LSTM cell looks exactly like a regular cell, except that its state is 
split into two vectors: h_(t) and c_(t) You can think of h(t) as the short-term
state and c(t) as the long-term state.

The key idea is that the network can learn what to
store in the long-term state, what to throw away, and what to read from it.

As the long-term state c_(t-1) traverses the network from left to right, you
can see that it first goes through a forget gate, dropping some memories,
and then it adds some new memories via the addition operation. 

The result c_(t) is sent straight out, without any further transformation. 
So, at each time step, some memories are dropped and some memories are
added. 

After the addition operation, the long-term state is copied and passed through
the tanh function, and then the result is filtered by the output gate. This
produces the short-term state h_(t). 

the current input vector x(t) and the previous short-term state h(t-1)
are fed to four different fully connected layers. 

- The main layer is the one that outputs g(t). It has the usual role of
analyzing the current inputs x(t) and the previous (short-term)
state h(t-1). Its output goes straight out to y(t) and h(t). 
In an LSTM cell this layer's output does not go straight out, but instead
its most important parts are stored in the long-term state.

- The three other layers are gate controllers. 

*The forget gate* (controlled by f(t)) controls which parts of
the long-term state should be erased.

*The input gate* (controlled by i(t)) controls which parts of
g(t) should be added to the long-term state.

*The output gate* (controlled by o(t)) controls which
parts of the long-term state should be read and output at
this time step, both to h(t) and to y(t).

An LSTM cell can learn to recognize an important input, store it in the
long-term state, preserve it for as long as it is needed, and extract it
whenever it is needed. This explains why these cells have been amazingly
successful at capturing long-term patterns in time series, long texts, audio
recordings, and more.
** Peephole connections
The previous long-term state c_(t-1) is added as an input to the
controllers of the forget gate and the input gate, and the current long-term
state c_(t) is added as input to the controller of the output gate. This often
improves performance, but not always, and there is no clear pattern for
which tasks are better off with or without them: you will have to try it on
your task and see if it helps.
** GRU cells
Both state vectors are merged into a single vector h(t). A single gate
controller z(t) controls both the forget gate and the input gate. If the gate
controller outputs a 1, the forget gate is open (= 1) and the input gate is
closed (1 - 1 = 0).

If it outputs a 0, the opposite happens. In other words, whenever a memory must
be stored, the location where it will be stored is erased first. There is a new
gate controller r(t) that controls which part of the previous state will be
shown to the main layer (g(t)).




LSTM and GRU cells are one of the main reasons behind the success of
RNNs. Yet while they can tackle much longer sequences than simple
RNNs, they still have a fairly limited short-term memory, and they have a
hard time learning long-term patterns in sequences of 100 time steps or
more, such as audio samples, long time series, or long sentences. One way
to solve this is to shorten the input sequences, for example using 1D
convolutional layers.
** Using 1D convolutional layers to process sequences
Similarly, a 1D convolutional
layer slides several kernels across a sequence, producing a 1D feature map
per kernel. Each kernel will learn to detect a single very short sequential
pattern. If you use 10 kernels, then the
layer’s output will be composed of 10 1-dimensional sequences, or equivalently you can view this output as a single 10-
dimensional sequence. 

If you use a 1D convolutional layer with a stride
of 1 and "same" padding, then the output sequence will have the same
length as the input sequence. 

But if you use "valid" padding or a stride
greater than 1, then the output sequence will be shorter than the input
sequence, so make sure you adjust the targets accordingly. 

#+begin_src python :result outputs
model = keras.models.Sequential([
keras.layers.Conv1D(
filters=20, 
kernel_size=4, 
strides=2,
padding="valid",
input_shape=[None, 1]),
keras.layers.GRU(20, return_sequences=True),
keras.layers.GRU(20, return_sequences=True),
keras.layers.TimeDistributed(keras.layers.Dense(10))
])

model.compile(loss="mse", optimizer="adam", metrics=[last_time_step_mse])

history = model.fit(
X_train, 
Y_train[:, 3::2], 
epochs=20,
validation_data=(X_valid, Y_valid[:, 3::2])
)

#+end_src 

* WaveNet
They stacked 1D convolutional
layers, doubling the dilation rate at every layer: the first convolutional layer gets a glimpse of just two
time steps at a time, while the next one sees four time steps, the next one sees
eight time steps, and so on. 

This way, the lower layers learn short-term patterns,
while the higher layers learn long-term patterns. 

The network can process extremely large sequences very efficiently.

with these dilation rates will act like a super-efficient convolutional layer
with a kernel of size 1,024 (except way faster, more powerful, and using
significantly fewer parameters), which is why they stacked 3 such blocks.
They also left-padded the input sequences with a number of zeros equal to
the dilation rate before every layer, to preserve the same sequence length
throughout the network. Here is how to implement a simplified WaveNet

#+begin_src python :result outputs
model = keras.models.Sequential()
model.add(keras.layers.InputLayer(input_shape=[None, 1]))
for rate in (1, 2, 4, 8) * 2:
    model.add(keras.layers.Conv1D(filters=20, kernel_size=2,
padding="causal",
activation="relu", dilation_rate=rate))
model.add(keras.layers.Conv1D(filters=10, kernel_size=1))
model.compile(loss="mse", optimizer="adam", metrics=[last_time_step_mse])
history = model.fit(X_train, Y_train, epochs=20,
    validation_data=(X_valid, Y_valid))
#+end_src



