
* Recurrent Neurons and Layers
- A recurrent neural network has connections pointing backward.
- *Unrolling The Network Through Time*
- At each time step t, every neuron receives both the input vector x_(t) and the output vector from the previous time step y_(t-1).
- Each recurrent neuron has two sets of weights:
- w_x for the inputs x_(t) 
- w_y for the outputs of the previous time step y_(t-1) 
- y_(t) = ϕ(W^T_x x_(t) + W_y^T y_(t-1) + b) = ϕ([X_(t) Y_(t−1)]W+b) with W = [W_x W_y]^T
- Y_(t) is an m × n_neurons (m is the number of instances in the mini-batch)
- X_(t) is an m × n_inputs 
- W_x is an n_inputs × n_neurons 
- W_y is an n_neurons × n_neurons 
- b is a vector of size n_neurons 
- Y_(t) = f(X_{0}, X_1, ... , X_t) 

* Memory Cells
- The output of a recurrent neuron at time step t is a function of all the inputs from previous time steps
- It has a form of memory.
- A part of a neural network that preserves some state across time steps is called a memory cell.
- h_(t) = f(h_(t–1), x_(t)).

*  Input and Output Sequences

*Sequence-to-Sequence Network*
- An RNN can simultaneously take a sequence of inputs and produce a sequence of outputs. 

*Sequence-to-Vector Network*
- Feed the network a sequence of inputs and ignore all outputs except for the last one. 

*Vector-to-Sequence Network*
- weed the network the same input vector over and over again at each time step and let it output a sequence.

*Encoder–Decoder*
- A sequence-to-vector network, called an encoder, followed by a vector-to-sequence network, called a decoder. 

* Training RNNs
- To train an RNN, the unroll it through time

- And then simply use regular backpropagation (backpropagation through time (BPTT))

- There is a first forward pass through the unrolled network. 

- Then the output sequence is evaluated using a cost function 
  C(Y_(0), Y_(1), …Y_(T)).
  
- The gradients of that cost function are then propagated backward through the
  unrolled network. 

- Finally the model parameters are updated using the gradients computed during BPTT. 

- wince the same parameters W and b are used at each time step, backpropagation will do the right thing and sum over all time steps.

* Forecasting a Time Series
 
- Using a time series generated by the
#+begin_src python :result outputs
def generate_time_series(batch_size, n_steps):
    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)
    time = np.linspace(0, 1, n_steps)
    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10)) 
    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) 
    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)
    return series[..., np.newaxis].astype(np.float32)

n_steps = 50
series = generate_time_series(10000, n_steps + 1)
X_train, y_train = series[:7000, :n_steps], series[:7000, -1]
X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]
X_test, y_test = series[9000:, :n_steps], series[9000:, -1]

#+end_src

- The function creates as many time series  of shape [batch size, time steps, 1]
- Each series is the sum of two sine waves of fixed amplitudes but random frequencies and phases,
- Plus a bit of noise.

* Baseline Metrics
- Predict the last value in each series (naive forecasting)
- Sometimes surprisingly difficult to outperform
- MSE 0.020
- Alternatively, use a fully connected network
- Add a Flatten layer
- Use a simple Linear Regression model
- Each prediction will is a linear combination of the values in the time series
- Compile the model using the MSE loss and Adam optimizer
- Then fit it on the training set for 20 epochs and evaluate it on the validation set
- MSE = 0.004
- Much better than the naive approach
#+begin_src python :result outputs
y_pred = X_valid[:, -1]
np.mean(keras.losses.mean_squared_error(y_valid, y_pred))
#0.020211367
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[50, 1]),
    keras.layers.Dense(1)])
#+end_src

* Implementing a Simple RNN
- RNN with a single layer (single neuron).  
- No need to specify the length of the input sequences (RNN can process any number of time steps)
- SimpleRNN layer uses the hyperbolic tangent activation function
- Initial state h_(init) is set to 0, and it is passed to a single recurrent neuron, along with the value of the first time step, x_(0).
- The neuron computes a weighted sum of these values and applies the hyperbolic tangent activation function to the result
- This gives the first output y_0
- In a simple RNN, this output is also the new state h_0.
- h_0 is passed to the same neuron along with  x_(1)
- The process continues until the last time step. 
- MSE = 0.014
- Better than the naive approach
- Does not beat a simple linear model
- For each neuron, linear model has one parameter per input and per time step, plus a bias term.
- For each recurrent neuron in a simple RNN, there is one parameter per input and per hidden state dimension, plus a bias term. 
- In a simple RNN, that's a total of just three parameters.

#+begin_src python :result outputs
model = keras.models.Sequential([
keras.layers.SimpleRNN(1, input_shape=[None, 1])
])
#+end_src
* Deep RNNs
- Use three SimpleRNN layers
- the last layer is not ideal
- It must have a single unit for forecasting a univariate time series
- A single output value per time step
- Hidden state is just a single number. 
- Not that useful
- the RNN uses the hidden states of the other recurrent layers to carry over information.
- Will not use the final layer's hidden state very much. 
- What if you want to use another activation function?
- Replace the output layer with a Dense layer
- Runs slightly faster
- The accuracy would be roughly the same
- Allows us to choose any activation function. 
- Converges faster and performs just as well

#+begin_src python :result outputs

model = keras.models.Sequential([
keras.layers.SimpleRNN(
          20, 
          return_sequences=True, input_shape=[None, 1]),
          keras.layers.SimpleRNN(20, return_sequences=True),
          keras.layers.SimpleRNN(1)
])

model = keras.models.Sequential([
keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),
keras.layers.SimpleRNN(20),
keras.layers.Dense(1)
])

#+end_src

* Forecasting Several Time Steps Ahead
- First option is to use already trained model for a single output
- Make it predict the next value
- Add that value to the inputs
- Use the model again to predict the following value
- And so on
- Prediction for next step is more accurate than it is for later time steps
- The errors might accumulate. 
- Much higher than the previous models
- Also a much harder task, so no comparison. 
- The naive approach is terrible
- The linear model is much better than using our RNN to forecast the future one step at a time
- Also much faster to train and run. 
- The second option is to train an RNN to predict next values at once (sequence-to-vector model)
- wt will output 10 values instead of 1. 
- Alternatively, train it to forecast the next 10 values at each and every time step
- The loss will contain a term for the output of the RNN at each and every time step.
- Many more error gradients flowing through the model
- They won't have to flow only through time
- They will also flow from the output of each time step
- This stabilize and speed up training.
- At time step 0 the model output a vector for time steps 1 to 10
- At time step 1 the model will forecast time steps 2 to 11, and so on. 
- Targets contain values that appear in the inputs
- At each time step, the model only knows about past time steps, so it cannot look ahead.
- It is said to be a causal model.
- To turn it into a sequence-to-sequence model, set return_sequences=True in all recurrent layers
- Apply the output Dense layer at every time step.
- The Dense layer supports sequences as inputs. 


#+begin_src python :result outputs
series = generate_time_series(1, n_steps + 10)
X_new, Y_new = series[:, :n_steps], series[:, n_steps:]
X = X_new
for step_ahead in range(10):
    y_pred_one = model.predict(X[:, step_ahead:])[:, np.newaxis, :]
    X = np.concatenate([X, y_pred_one], axis=1)
    Y_pred = X[:, n_steps:]

# output 10 values at once
series = generate_time_series(10000, n_steps + 10)
X_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0]
X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]
X_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0]
#Now we just need the output layer to have 10 units instead of 1:
model = keras.models.Sequential([
keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None,
1]),
keras.layers.SimpleRNN(20),
keras.layers.Dense(10)
])
#After training , you can predict the next 10 values at once very easily:
Y_pred = model.predict(X_new)


# sequence-to sequence
Y = np.empty((10000, n_steps, 10)) # each target is a sequence of 10D vectors
for step_ahead in range(1, 10 + 1):
    Y[:, :, step_ahead - 1] = series[:, step_ahead:step_ahead + n_steps, 0]
    Y_train = Y[:7000]
    Y_valid = Y[7000:9000]
    Y_test = Y[9000:]NOTE
#
model = keras.models.Sequential([
          keras.layers.SimpleRNN(
          20, 
          return_sequences=True, 
          input_shape=[None, 1]),
          keras.layers.SimpleRNN(20, return_sequences=True),
          keras.layers.TimeDistributed(keras.layers.Dense(10))
])

#
def last_time_step_mse(Y_true, Y_pred):
    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])
optimizer = keras.optimizers.Adam(lr=0.01)
model.compile(loss="mse", optimizer=optimizer, metrics= [last_time_step_mse])
#+end_src
* Handling Long Sequences
- Run it over many time steps
- Make the unrolled RNN a very deep network. 
- May suffer from the unstable gradients problem
- May be unstable
- RNN gradually forgets the first inputs in the sequence 
* Fighting the Unstable Gradients Problem
- Nonsaturating activation functions may lead to unstability.
- Using a smaller learning rate to avoid exploring gradients
- Alternatively, use a saturating activation function (hyperbolic tangent) 
- Batch Normalization cannot be used as efficiently with RNNs. 
- BN was slightly beneficial when applied to the inputs, not to the hidden states
- In Keras, BatchNormalization layer before each recurrent layer (don’t expect too much) 
- Layer Normalization works better
- It normalizes across the features dimension
- It can compute the required statistics on the fly, at each time step
- Layer Normalization learns a scale and an offset parameter for each input
- Typically used right after the linear combination of the inputs and the hidden states 

#+begin_src python :result outputs
class LNSimpleRNNCell(keras.layers.Layer):

    def __init__(self, units, activation="tanh", **kwargs):
        super().__init__(**kwargs)
        self.state_size = units
        self.output_size = units
        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units,
        activation=None)
        self.layer_norm = keras.layers.LayerNormalization()
        self.activation = keras.activations.get(activation)

    def call(self, inputs, states):
        outputs, new_states = self.simple_rnn_cell(inputs, states)
        norm_outputs = self.activation(self.layer_norm(outputs))
        return norm_outputs, [norm_outputs]
model = keras.models.Sequential([
keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True,
input_shape=[None, 1]),
keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),
keras.layers.TimeDistributed(keras.layers.Dense(10))
])
#+end_src
* Tackling the Short-Term Memory Problem
Due to the transformations that the data goes through when traversing an
RNN, some information is lost at each time step. After a while, the RNN's
state contains virtually no trace of the first inputs. 
** LSTM cells
If you consider the LSTM cell as a black box, it can
be used very much like a basic cell, except it will perform much better;
training will converge faster, and it will detect long-term dependencies in
the data. In Keras, you can simply use the LSTM layer instead of the
SimpleRNN layer:
#+begin_src python :result outputs
model = keras.models.Sequential([
keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),
keras.layers.LSTM(20, return_sequences=True),
keras.layers.TimeDistributed(keras.layers.Dense(10))
])
# Alternatively, you could use the general-purpose keras.layers.RNN layer,
# giving it an LSTMCell as an argument:
model = keras.models.Sequential([
keras.layers.RNN(keras.layers.LSTMCell(20), return_sequences=True,
input_shape=[None, 1]),
keras.layers.RNN(keras.layers.LSTMCell(20), return_sequences=True),
keras.layers.TimeDistributed(keras.layers.Dense(10))
])
#+end_src

The LSTM layer uses an optimized implementation when running
on a GPU, so in general it is preferable to use it.

The LSTM cell looks exactly like a regular cell, except that its state is 
split into two vectors: h_(t) and c_(t) You can think of h(t) as the short-term
state and c(t) as the long-term state.

The key idea is that the network can learn what to
store in the long-term state, what to throw away, and what to read from it.

As the long-term state c_(t-1) traverses the network from left to right, you
can see that it first goes through a forget gate, dropping some memories,
and then it adds some new memories via the addition operation. 

The result c_(t) is sent straight out, without any further transformation. 
So, at each time step, some memories are dropped and some memories are
added. 

After the addition operation, the long-term state is copied and passed through
the tanh function, and then the result is filtered by the output gate. This
produces the short-term state h_(t). 

the current input vector x(t) and the previous short-term state h(t-1)
are fed to four different fully connected layers. 

- The main layer is the one that outputs g(t). It has the usual role of
analyzing the current inputs x(t) and the previous (short-term)
state h(t-1). Its output goes straight out to y(t) and h(t). 
In an LSTM cell this layer's output does not go straight out, but instead
its most important parts are stored in the long-term state.

- The three other layers are gate controllers. 

*The forget gate* (controlled by f(t)) controls which parts of
the long-term state should be erased.

*The input gate* (controlled by i(t)) controls which parts of
g(t) should be added to the long-term state.

*The output gate* (controlled by o(t)) controls which
parts of the long-term state should be read and output at
this time step, both to h(t) and to y(t).

An LSTM cell can learn to recognize an important input, store it in the
long-term state, preserve it for as long as it is needed, and extract it
whenever it is needed. This explains why these cells have been amazingly
successful at capturing long-term patterns in time series, long texts, audio
recordings, and more.
** Peephole connections
The previous long-term state c_(t-1) is added as an input to the
controllers of the forget gate and the input gate, and the current long-term
state c_(t) is added as input to the controller of the output gate. This often
improves performance, but not always, and there is no clear pattern for
which tasks are better off with or without them: you will have to try it on
your task and see if it helps.
** GRU cells
Both state vectors are merged into a single vector h(t). A single gate
controller z(t) controls both the forget gate and the input gate. If the gate
controller outputs a 1, the forget gate is open (= 1) and the input gate is
closed (1 - 1 = 0).

If it outputs a 0, the opposite happens. In other words, whenever a memory must
be stored, the location where it will be stored is erased first. There is a new
gate controller r(t) that controls which part of the previous state will be
shown to the main layer (g(t)).




LSTM and GRU cells are one of the main reasons behind the success of
RNNs. Yet while they can tackle much longer sequences than simple
RNNs, they still have a fairly limited short-term memory, and they have a
hard time learning long-term patterns in sequences of 100 time steps or
more, such as audio samples, long time series, or long sentences. One way
to solve this is to shorten the input sequences, for example using 1D
convolutional layers.
** Using 1D convolutional layers to process sequences
Similarly, a 1D convolutional
layer slides several kernels across a sequence, producing a 1D feature map
per kernel. Each kernel will learn to detect a single very short sequential
pattern. If you use 10 kernels, then the
layer’s output will be composed of 10 1-dimensional sequences, or equivalently you can view this output as a single 10-
dimensional sequence. 

If you use a 1D convolutional layer with a stride
of 1 and "same" padding, then the output sequence will have the same
length as the input sequence. 

But if you use "valid" padding or a stride
greater than 1, then the output sequence will be shorter than the input
sequence, so make sure you adjust the targets accordingly. 

#+begin_src python :result outputs
model = keras.models.Sequential([
keras.layers.Conv1D(
filters=20, 
kernel_size=4, 
strides=2,
padding="valid",
input_shape=[None, 1]),
keras.layers.GRU(20, return_sequences=True),
keras.layers.GRU(20, return_sequences=True),
keras.layers.TimeDistributed(keras.layers.Dense(10))
])

model.compile(loss="mse", optimizer="adam", metrics=[last_time_step_mse])

history = model.fit(
X_train, 
Y_train[:, 3::2], 
epochs=20,
validation_data=(X_valid, Y_valid[:, 3::2])
)

#+end_src 

* WaveNet
They stacked 1D convolutional
layers, doubling the dilation rate at every layer: the first convolutional layer gets a glimpse of just two
time steps at a time, while the next one sees four time steps, the next one sees
eight time steps, and so on. 

This way, the lower layers learn short-term patterns,
while the higher layers learn long-term patterns. 

The network can process extremely large sequences very efficiently.

with these dilation rates will act like a super-efficient convolutional layer
with a kernel of size 1,024 (except way faster, more powerful, and using
significantly fewer parameters), which is why they stacked 3 such blocks.
They also left-padded the input sequences with a number of zeros equal to
the dilation rate before every layer, to preserve the same sequence length
throughout the network. Here is how to implement a simplified WaveNet

#+begin_src python :result outputs
model = keras.models.Sequential()
model.add(keras.layers.InputLayer(input_shape=[None, 1]))
for rate in (1, 2, 4, 8) * 2:
    model.add(keras.layers.Conv1D(filters=20, kernel_size=2,
padding="causal",
activation="relu", dilation_rate=rate))
model.add(keras.layers.Conv1D(filters=10, kernel_size=1))
model.compile(loss="mse", optimizer="adam", metrics=[last_time_step_mse])
history = model.fit(X_train, Y_train, epochs=20,
    validation_data=(X_valid, Y_valid))
#+end_src



