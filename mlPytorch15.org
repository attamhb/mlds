* Modeling Sequential Data Using Recurrent Neural Networks
- Introducing sequential data
- RNNs for modeling sequences
- Long short-term memory
- Truncated backpropagation through time
- Implementing a multilayer RNN for sequence modeling in PyTorch
- Project one: RNN sentiment analysis of the IMDb movie review dataset
- Project two: RNN character-level language modeling with LSTM cells, using text data from Jules Verne’s The Mysterious Island
- Using gradient clipping to avoid exploding gradients
* Introducing sequential data

- Sequential data, sequence data or sequences

** Modeling sequential data – order matters

- Elements in a sequence appear in a certain order and are not independent of each other.

- (Supervised learning) the input IID data

- The training examples are mutually independent and have the same underlying distribution.

- The order in which the training examples are given to the model is irrelevant.

- This assumption is not valid when we deal with sequences—by definition, order matters.

- Predicting the market value of a particular stock

** Sequential data versus time series data

- Time series data is a special type of sequential data where each example is associated with a dimension for time.

- The time dimension determines the order among the data points.

- Not all sequential data has the time dimension (text data or DNA sequences).

** Representing sequences

- RNNs are designed for modeling sequences and are capable of remembering past information and processing new events accordingly. 
** The different categories of sequence modeling

- Sequence modeling has many fascinating applications.

- If either the input or output is a sequence, the modeling task likely falls into one of these categories

- Many-to-one (input: sequence, output: vector or scalar) Sentiment analysis

- One-to-many (input: standard format, output: sequence) image captioning

- Many-to-many (input: sequences, output: sequences)

- Many-to-many (Synchronized)  Video classification, where each frame in a video is labeled.

- Many-to-many (Delayed) Translating one language into another
  
* RNNs for modeling sequences

- RNN includes a recursive component to model sequence data

** Understanding the dataflow in RNNs

- In a standard feedforward network, information flows from the input to the hidden layer, and then from the hidden layer to the output layer.

- In an RNN, the hidden layer receives its input from both the input layer of the current time step and the hidden layer from the previous time step.

- The flow of information in adjacent time steps in the hidden layer allows the network to have a memory of past events.

- This flow of information is displayed as a loop (recurrent edge). 

- At the first time step, t = 0, the hidden units are initialized to zeros or small random values.

- The behavior of the last recurrent layer depends on the type of problem.

** Computing activations in an RNN

- Consider just a single hidden layer RNNs

- Each directed edge is associated with a weight matrix

- Those weights do not depend on time

- They are shared across the time axis

- W_xh: Between the input and the hidden layer
- W_hh: associated with the recurrent edge
- W_ho: Between the hidden layer and output layer

** Hidden recurrence versus output recurrence
- There is an alternative model in which the recurrent connection comes from the output layer.

- In this case, the net activations from the output layer at the previous time step can be added in one of two ways

- To the hidden layer at the current time step

- To the output layer at the current time step

- The differences between these architectures can be clearly seen in the recurring connections

- The weights associated with the recurrent connection will be denoted for the 
  -- hidden-to-hidden recurrence by W_hh

  -- output-to-hidden recurrence by W_oh

  -- output-to-output recurrence by W_oo

- Manually compute the forward pass for one of these recurrent types.


*  The challenges of learning long-range interactions

- BPTT introduces some new challenges

- Vanishing and exploding gradient problems arise

- Three solutions (Gradient clipping. TBPTT,LSTM)

-- Gradient clipping: Specify threshold value for the gradients
-- TBPTT: limit the number of time steps for backpropagation 
-- LSTM: successful in vanishing and exploding gradient problems 

** Long short-term memory cells (LSTM)
- LSTMs were first introduced to overcome the vanishing gradient problem

- The building block of an LSTM is a memory cell, which essentially represents or replaces the hidden layer of standard RNNs.

- In each memory cell, there is a recurrent edge that has the desirable weight, w = 1 to overcome the vanishing and exploding gradient problems.

- The values associated with this recurrent edge are collectively called the cell state.

- The cell state from the previous time step is modified to get the cell state at the current time step, without being multiplied directly by any weight factor

- The flow of information in this memory cell is controlled by several computation units (often called gates) that will be described here.

- Four boxes are indicated with an activation function

- These boxes apply a linear combination by performing matrix-vector multiplications on their inputs

- These units of computation with sigmoid activation functions are called gates.

- Three different types of gates (forget gate, input gate, the output gate).

*  Truncated backpropagation through time
*  Implementing a multilayer RNN for sequence modeling in PyTorch
- Sentiment analysis
- Language modeling
*  Project one: RNN sentiment analysis of the IMDb movie review dataset
** Intro 
- Implement a multilayer RNN for sentiment analysis 

- Import the necessary modules and read the data from torchtext.

- Train and Test: Each set has 25,000 samples.

- Each sample of the datasets consists of two elements

- The sentiment label and  The movie review text.

- Split the training set into separate training and validation partitions (20,000 and 5,000)

- Identify the unique words in the training dataset

- Map each unique word to a unique integer and encode into integers 

- Divide the dataset into mini-batches as input to the model

- The original training dataset contains 25,000 examples.

- Encode the data into numeric values

- First find the unique words (tokens) in the training dataset.

- Use the Counter class from the collections package to tokenize

- To split the text into words (tokens), use the tokenizer function, and remove HTML markups,  punctuation and other non-letter characters

- Map each unique word to a unique integer.

- The torchtext package provides a class, Vocab, which we can use to create such a mapping and encode the entire dataset.

- Create a vocab object by passing the ordered tokens to their corresponding frequencies.

- Add padding and the unknown tokens

- Define the text pipeline and the label pipeline 

- Generate batches of samples using DataLoader and pass the data processing pipelines 

- Wrap the text encoding and label transformation function into the function

- The sequences currently have different lengths.

- Make sure that all the sequences in a mini-batch have the same length to store them efficiently in a tensor.
** Embedding layers for sentence encoding

- Map each word to a vector of a fixed size with real-valued elements (not necessarily integers).

- Can use finite-sized vectors to represent an infinite number of real numbers.

- Given the number of unique words, nwords, we can select the size of the embedding  dimension to be much smaller than the number of unique words to represent the entire vocabulary as input features.

- A reduction in the dimensionality of the feature space to decrease the effect of the curse of dimensionality

- The extraction of salient features since the embedding layer in an NN can be optimized (or learned)


- An embedding matrix of size (n + 2) × embedding_dim will be created where each row

- The embedding matrix serves as the input layer to our NN models.
** Building an RNN model
- Using the nn.Module class
- Can combine the embedding layer, the recurrent layers of the RNN, and the fully connected non-recurrent layers
- For the recurrent layers, can use a regular RNN, LSTM: GRU
#+begin_src python :result outputs

class RNN(nn.Module):

    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.rnn = nn.RNN(input_size, hidden_size, num_layers=2,
                       batch_first=True)

    self.fc = nn.Linear(hidden_size, 1) 

    def forward(self, x):
        _, hidden = self.rnn(x)
        out = hidden[-1, :, :] 
        out = self.fc(out)
        return out

#+end_src
** Building an RNN model for the sentiment analysis task
- Use an LSTM layer to account for long-range effects

- Create an RNN model for sentiment analysis

- Starting with an embedding layer producing word embeddings of feature size 20.

- Add a recurrent layer of type LSTM

- Add a fully connected layer as a hidden layer and another fully connected layer as the output layer

- Use sigmoid activation as the prediction

- Develop the train function to train the model on the given dataset for one epoch and return the classification accuracy and loss

- Develop the evaluate function to measure the model's performance on a given dataset

- Create a loss function and optimizer

- Train the model for 10 epochs and display the training and validation performances

- After training this model for 10 epochs, we will evaluate it on the test data

** More on the bidirectional RNN
- Set the bidirectional configuration of the LSTM to True
- This will make the recurrent layer pass through the input sequences from both directions, start to end, as well as in the reverse direction
- The bidirectional RNN layer makes two passes over each input sequence
- A forward pass and a reverse or backward pass.
- The resulting hidden states of these forward and backward passes are usually concatenated into a single hidden state.

*  Project two: RNN character-level language modeling with LSTM cells, using text data from Jules Verne’s The Mysterious Island

** Preprocessing the dataset

- Generating Text with RNN

- Input: text document

- Goal: Generate new text similar in style to the input  using some ML model.

- Preprocess: Remove unnecessary portions 

- Set of unique characters observed in the text

- Tokenize

- A classification task

- Predict the next character, starting with a sequence of length 1 

- Clip the sequence length to 40.

- Split the text into chunks of size 41

- 40 x, 1 y

** Building a character-level RNN model

- Output logits to sample from predictions to generate new text

- Specify the model parameters and create an RNN model

- Create a loss function (Adam optimizer)

- Optimizer (cross-entropy-loss)

- Train the model 

- In each epoch, use only one batch randomly chosen from the data loader

- Display the training loss for every 500 epochs

** Evaluation phase – generating new text passages

- Cnvert logits into probabilities through softmax function

- Select the element with the maximum logit value to predict the next character

- Sample randomly from the predictions to produce new text.

- Define sample() to receive a short starting string  and generate a new string
