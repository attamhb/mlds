
*  CNNS
IBM’s Deep Blue supercomputer beat the chess world champion Garry Kasparov back in 1996

CNNs emerged from the study of the brain’s visual cortex, and they have been
used in image recognition since the 1980s. 

Due to the increase in computational power, the amount of available training
data, CNNs have managed to achieve superhuman performance on some complex visual tasks. 

They power image search services, 
- Self-driving cars, 
- Automatic video classification
- Systems, 
- Voice recognition 
- Natural language processing. 

* Convolutional Layers

The convolutional layer is the fundamental building block of a CNN, playing a
crucial role in its architecture.

Rather than being connected to every single pixel in the input image, neurons in
the first convolutional layer establish connections solely with pixels within
their respective receptive fields.

In the subsequent convolutional layer, each neuron forms connections exclusively
with neurons positioned in a small rectangular region of the previous layer.

This design enables the network to focus on extracting small-scale, low-level
features in the initial hidden layer, which are then progressively combined to
form larger, high-level features in subsequent layers.

One notable characteristic of CNNs is that each layer is represented in a 2D
format, facilitating the correspondence between neurons and their corresponding
inputs.

To illustrate, a neuron situated at row 'i' and column 'j' in a given layer
establishes connections with the outputs of neurons in the previous layer,
specifically within the range of:

| Position of a neuron | Connections in the previous layer |
| (i,j)                | (i:i+f_h-1, j:j+f_w-1)              |

Here, 'f_h' and 'f_w' denote the height and width of the receptive field, respectively.
** *Padding*
In order for a layer to have the same height and width as the previous layer, 
it is common to add zeros around the inputs. This is called zero padding.

** *Strides*
It is also possible to connect a large input layer to a much smaller layer by
spacing out the receptive fields, as shown in Figure 14-4. This
dramatically reduces the model’s computational complexity. The shift
from one receptive field to the next is called the stride. 

| Position of a neuron | Connections in the previous layer |
| (i,j)                | (i*s_h:i*s_h+f_h-1,j:j*s_w+f_w-1)      |
where s_h and s_w are the vertical and horizontal strides.

** Filters (Convolutional Kernel) 
A neuron's weights can be represented as a small image the size of the
receptive field, called *filters* or *convolution kernels*. 
Thus, a layer full of neurons using the same filter outputs a *feature map*, 
which highlights the areas in an image that activate the filter the most. 

You do not have to define the filters manually. 
During training the convolutional layer will automatically learn the most useful
filters for its task, and the layers above will learn to combine them into more
complex patterns.

In reality a convolutional layer has multiple filters and outputs 
*one feature map per filter*. 

It has one neuron per pixel in each feature map, 

All neurons within a given feature map share the same parameters. 

A neuron’s receptive field is the same as described earlier, but it extends
across all the previous layers’ feature maps. 

A convolutional layer simultaneously applies multiple trainable
filters to its inputs, making it capable of detecting multiple features anywhere
in its inputs.

The fact that all neurons in a feature map share the same parameters dramatically
reduces the number of parameters in the model. 

Once the CNN has learned to recognize a pattern in one location, it can
recognize it in any other location.

|------------------------+----------------------------------|
| layer l, feature map k | layer l-1                        |
|------------------------+----------------------------------|
| (i,j)                  | (ixs_h:ixs_h+f_h-1, jxs_w:jxs_w+f_w-1) |
|------------------------+----------------------------------|

* Memory Requirements
During the reverse pass of backpropagation, it is necessary to have access to
all the intermediate values that were calculated during the forward pass.

In the event of a training failure caused by an out-of-memory error, there are
several approaches you can take to address the issue. One option is to reduce
the size of the mini-batches used in training. By decreasing the number of
examples processed in each iteration, you can mitigate memory constraints.

Another strategy is to reduce the dimensionality of the network. This can be
achieved by employing a stride, which decreases the spatial resolution of
feature maps, or by removing a few layers from the architecture. Both methods
help reduce the memory requirements.

Switching from 32-bit floating-point precision to 16-bit floating-point
precision is another potential solution. This reduces the memory footprint of
the network at the cost of some loss in numerical precision.

Alternatively, if available, you can distribute the CNN across multiple
devices. This approach allows you to utilize the memory resources of multiple
hardware devices in parallel, alleviating the memory limitations.

By employing these techniques, you can tackle out-of-memory errors during
training and continue the optimization process of your CNN.
* Pooling Layers
Each neuron within a pooling layer establishes connections exclusively with a
limited number of neurons from the previous layer. These connections are formed
within a small rectangular receptive field, the dimensions of which are
determined by the size, stride, and padding type used in the pooling
operation. It's important to note that pooling layers do not involve weights.

In addition to its connectivity pattern, a max pooling layer introduces a degree
of invariance to small translations. This means that the layer's output remains
relatively unchanged even if the input undergoes slight shifts in position.

By incorporating max pooling layers at regular intervals within a CNN
architecture, it becomes possible to achieve a certain level of translation
invariance on a larger scale. This means that the network becomes less sensitive
to small positional variations of features.

Moreover, max pooling also offers a limited amount of rotational invariance and
slight scale invariance. This implies that the network's predictions are less
affected by moderate rotations or variations in the size of the input.

These types of invariances provided by max pooling can be advantageous in
scenarios where the prediction should not rely heavily on precise details, such
as in classification tasks. They allow the network to focus more on the overall
features and patterns rather than being overly influenced by specific positional
or scaling factors.

* TensorFlow Implementation

By default, the stride in max pooling layers is set to the size of the pooling
kernel. For example, in Keras, you can create a max pooling layer with a pool
size of 2 using the following code:
#+begin_src  python
max_pool = keras.layers.MaxPool2D(pool_size=2)
#+end_src  

The depthwise max pooling layer is not available in Keras, but TensorFlow's
low-level Deep Learning API does include this functionality. For instance, you
can use the tf.nn.max_pool function to perform depthwise max pooling on
images. The following code snippet demonstrates its usage:

#+begin_src  python
output = tf.nn.max_pool(
                        images,
                        ksize=(1, 1, 1, 3),
                        strides=(1, 1, 1, 3),
                        padding="valid"
)
#+end_src  python


You can include this as a layer in your Keras models

#+begin_src  python
depth_pool = keras.layers.Lambda(
                  lambda X: tf.nn.max_pool(
                                           X, 
                                           ksize=(1, 1, 1, 3), 
                                           strides=(1, 1, 1, 3),
                                           padding="valid"
))
#+end_src  python

Global average pooling layer. 

#+begin_src  python
global_avg_pool = keras.layers.GlobalAvgPool2D()
#+end_src  python
It’s equivalent to this simple Lambda layer, which computes the mean over
the spatial dimensions (height and width):

#+begin_src  python
global_avg_pool = keras.layers.Lambda(lambda X: tf.reduce_mean(X, axis= [1, 2]))
#+end_src  python

* Tensor Flow Implementation 
Input image: [height, width, channels]. 
A mini-batch: [mini-batch size, height, width, channels]. 
Weights of convolutional layer: [f_h, f_w, f_n′, f_n].
The bias terms of a convolutional layer:  1D tensor of shape [f_n].

#+begin_src python   :results output
from sklearn.datasets import load_sample_image
china = load_sample_image("china.jpg") / 255
flower = load_sample_image("flower.jpg") / 255
images = np.array([china, flower])
batch_size, height, width, channels = images.shape
filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)
filters[:, 3, :, 0] = 1 # vertical line
filters[3, :, :, 1] = 1 # horizontal line
outputs = tf.nn.conv2d(images, filters, strides=1, padding="same")
plt.imshow(outputs[0, :, :, 1], cmap="gray") # plot 1st image's 2nd
feature map
plt.show()
#+end_src 

Strides is equal to 1, 
Strides could could also be a 1D array with four elements, 
where the two central elements are the vertical and horizontal strides 
(s_h and s_w). 
The first and last elements must currently be equal to 1.

If set to "valid", the convolutional layer does not use zero padding and may
ignore some rows and columns at the bottom and right of the input image,
depending on the stride. This means that every neuron’s receptive field lies
strictly within valid positions inside the input, hence the name valid.

In a real CNN you would normally define filters as trainable variables so 
the neural net can learn which filters work best.

#+begin_src python 
conv = keras.layers.Conv2D(filters=32, 
                           kernel_size=3, 
                           strides=1,
                           padding="same", 
                           activation="relu")
#+end_src 
* CNN Architectures
Typical CNN architectures stack a 
few convolutional layers (each one generally followed by a ReLU layer), 
then a pooling layer, 
then another few convolutional layers (+ReLU), 
then another pooling layer, 
and so on.

The image gets smaller and smaller as it progresses through the network,
but it also typically gets deeper and deeper. 

At the top of the stack, a regular feedforward neural network is added, 
composed of a few fully connected layers (+ReLUs), 

and the final layer outputs the prediction.

A common mistake is to use convolution kernels that are too large. 
Smaller kernels use fewer parameters and require fewer computations, and it will
usually perform better. One exception is for the first convolutional layer

#+begin_src python 
model = keras.models.Sequential([
keras.layers.Conv2D(64, 7, activation="relu", padding="same", input_shape=[28, 28, 1]),
keras.layers.MaxPooling2D(2),
keras.layers.Conv2D(128, 3, activation="relu", padding="same"),
keras.layers.Conv2D(128, 3, activation="relu", padding="same"),
keras.layers.MaxPooling2D(2),
keras.layers.Conv2D(256, 3, activation="relu", padding="same"),
keras.layers.Conv2D(256, 3, activation="relu", padding="same"),
keras.layers.MaxPooling2D(2),
keras.layers.Flatten(),
keras.layers.Dense(128, activation="relu"),
keras.layers.Dropout(0.5),
keras.layers.Dense(64, activation="relu"),
keras.layers.Dropout(0.5),
keras.layers.Dense(10, activation="softmax")])
#+end_src

This CNN reaches over 92% accuracy on the test set. 
* LeNet-5
It was created by Yann LeCun in 1998 and has been widely used for handwritten
digit recognition (MNIST). 
* AlexNet

The AlexNet CNN architecture, developed by Alex Krizhevsky et al., achieved
significant success in the 2012 ImageNet ILSVRC challenge, with a top-five error
rate of 17%. One of the notable aspects of AlexNet was its introduction of
stacking convolutional layers directly on top of one another, which was a
departure from previous approaches.

To address the issue of overfitting, the authors incorporated two regularization
techniques in AlexNet.

The first technique employed was dropout, where during training, randomly
selected neurons in a layer are "dropped out" or temporarily ignored. In
AlexNet, a dropout rate of 50% was applied, meaning that each neuron had a 50%
chance of being dropped out during training. Dropout helps prevent co-adaptation
of neurons, forcing the network to rely on more robust and general features.

Additionally, AlexNet utilized a technique called local response normalization
(LRN) immediately after the Rectified Linear Unit (ReLU) activation step in
layers C1 and C3. LRN involves inhibiting neurons in a feature map that are
strongly activated and located at the same position as neurons in neighboring
feature maps. This competition between neurons encourages specialization and
diversification among feature maps, leading to a broader exploration of features
and improved generalization capabilities.

By incorporating dropout regularization and local response normalization,
AlexNet aimed to mitigate overfitting and enhance the network's ability to
generalize well to unseen data. These techniques played a crucial role in the
architecture's success in the ImageNet challenge.

*  GoogLeNet 

The GoogLeNet CNN architecture, developed by Christian Szegedy et al. from
Google Research, achieved remarkable success in the ILSVRC 2014 challenge with a
top-five error rate below 7%. It was designed to be much deeper than previous
CNNs and introduced the concept of inception modules, which enabled the network
to use parameters more efficiently.

An inception module in GoogLeNet consists of multiple parallel branches, each
performing a different type of convolutional operation. For example, a notation
like "3 × 3 + 1(S)" indicates a branch that uses a 3 × 3 kernel, stride 1, and
"same" padding. The input signal is first copied and fed into each branch
independently. All convolutional layers in the inception module use the
ReLU activation function.

The outputs from all branches are then concatenated along the depth dimension in
the final depth concatenation layer. The inception modules serve multiple
purposes. While they cannot capture spatial patterns, they excel at capturing
patterns along the depth dimension. Additionally, by outputting fewer feature
maps than their inputs, they act as bottleneck layers, reducing
dimensionality. Moreover, each pair of convolutional layers within an inception
module acts as a powerful convolutional layer, capable of capturing more complex
patterns. In essence, these pairs of convolutional layers sweep a two-layer
neural network across the image, akin to a simple linear classifier.

The number of convolutional kernels for each convolutional layer in the
inception module is a hyperparameter that can be adjusted. GoogLeNet consists of
nine inception modules, and the six numbers associated with each module
represent the number of feature maps output by each convolutional layer within
the module.

The initial layers of GoogLeNet divide the image's height and width by 4,
effectively reducing its area by 16. Subsequently, a local response
normalization layer ensures that the preceding layers learn a diverse range of
features. Following this, two convolutional layers are employed, where the first
acts as a bottleneck layer.

Due to the dimensionality reduction achieved by the bottleneck layer, it is
unnecessary to have multiple fully connected layers at the top of the CNN. This
significantly reduces the number of parameters in the network and mitigates the
risk of overfitting.

* VGGNet
It had a very simple and classical architecture, 
with 2 or 3 convolutional layers and a pooling layer, 
then again 2 or 3 convolutional layers and a pooling layer, 
and so on, 
plus a final dense network with 2 hidden layers and the output layer. 
It used only 3 × 3 filters, but many filters.
* ResNet

ResNet (short for Residual Network) is a type of deep convolutional neural
network that addresses the challenges of training very deep models with an
increasing number of parameters. It introduces skip connections, also known as
residual connections, to facilitate the learning process.

As models become deeper, they can encounter difficulties in convergence and
information flow. ResNet aims to overcome these challenges by learning residual
functions to model the target function, denoted as h(x), where x represents the
input. By adding the input x to the output of the network, ResNet forces the
network to learn the residual function f(x) = h(x) - x. This approach proves
beneficial when the target function is similar to the identity function, as it
speeds up training considerably.

The addition of skip connections allows the network to make progress even if
some layers are not yet effectively learning. These skip connections enable the
signal to propagate easily across the entire network, facilitating gradient flow
and reducing the vanishing gradient problem.

ResNet can be visualized as a stack of residual units, where each unit comprises
two convolutional layers. The number of feature maps is typically doubled every
few residual units, while their height and width are halved. However, due to the
change in dimensions, the input cannot be directly added to the output of the
residual unit. To address this issue, ResNet utilizes a 1 × 1 convolutional
layer with a stride of 2 and the appropriate number of output feature maps. This
step ensures that the input is transformed to match the dimensions of the output
before being added to the residual unit's output.

By incorporating skip connections and residual units, ResNet effectively trains
deeper models with improved information flow and faster convergence. It has
achieved impressive results in various computer vision tasks and has become a
popular architecture in the deep learning community.

* Xception

The Xception architecture merges ideas from both the GoogLeNet and ResNet
architectures but replaces the inception modules with a specialized layer called
a depthwise separable convolution layer. While separable convolution layers were
used in some earlier CNN architectures, they take on a central role in the
Xception architecture.

A depthwise separable convolution layer makes the assumption that spatial
patterns and cross-channel patterns can be modeled independently. It consists of
two parts. The first part involves applying a single spatial filter to each
input feature map, focusing on capturing spatial patterns. The second part
focuses on identifying cross-channel patterns, which capture relationships
between different channels of the input.

In the Xception architecture, the initial layers comprise regular convolutional
layers. However, for the remainder of the architecture, separable convolutions
are predominantly used, along with a few max pooling layers and the usual final
layers (e.g., fully connected layers).

One of the advantages of separable convolutional layers is that they require
fewer parameters, consume less memory, and involve fewer computations compared
to regular convolutional layers. Additionally, they often yield better
performance. As a result, it is recommended to consider using separable
convolutions as the default choice in CNN architectures.

The Xception architecture leverages depthwise separable convolution layers to
effectively capture spatial and cross-channel patterns while being
computationally efficient and parameter-efficient. This approach has
demonstrated impressive results in various computer vision tasks.

* SENet

The extended versions of Inception networks and ResNets, namely SE-Inception and
SE-ResNet, incorporate an additional neural network component called an SE block
into each unit of the original architectures.

The SE block is responsible for analyzing the output of the unit it is attached
to, focusing exclusively on the depth dimension (the channels), without
considering spatial patterns. Its purpose is to learn which features tend to be
active together. By leveraging this learned information, the SE block
recalibrates the feature maps to enhance their effectiveness.

For example, an SE block might learn that in images, mouths, noses, and eyes
typically appear together. If it observes strong activation in the mouth and
nose feature maps but only mild activation in the eye feature map, it will boost
the eye feature map (or, more accurately, reduce irrelevant feature maps). This
recalibration process helps resolve ambiguities and improve the overall
representation.

An SE block consists of three layers: a global average pooling layer, a hidden
dense layer with the ReLU activation function, and a dense output layer with the
sigmoid activation function. The global average pooling layer computes the mean
activation for each feature map, compressing the responses into a small vector
representation. This low-dimensional vector serves as an embedding of the
distribution of feature responses, capturing the combinations of features. The
output layer takes this embedding and generates a recalibration vector, with one
number per feature map, ranging from 0 to 1. The feature maps are then
multiplied by this recalibration vector, scaling down irrelevant features (with
low recalibration scores) while preserving relevant features (with recalibration
scores close to 1).

By incorporating SE blocks into the architecture, SE-Inception and SE-ResNet
models gain a boost in performance. The SE blocks allow the models to
dynamically recalibrate feature maps based on their interdependencies, improving
the representation power and enhancing the network's ability to capture complex
patterns and relationships.

*  Implementing a ResNet-34 CNN Using Keras
First, let’s create a ResidualUnit layer:
#+begin_src python :result outputs
class ResidualUnit(keras.layers.Layer):
    def __init__(self, filters, strides=1, activation="relu", **kwargs):
        super().__init__(**kwargs)
            self.activation = keras.activations.get(activation) 
                self.main_layers = [keras.layers.Conv2D(filters, 3, strides=strides, padding="same", use_bias=False), keras.layers.BatchNormalization(), self.activation, keras.layers.Conv2D(filters, 3, strides=1, padding="same", use_bias=False), keras.layers.BatchNormalization()]
            self.skip_layers = []
            if strides > 1:
                self.skip_layers = [keras.layers.Conv2D(filters, 1, strides=strides, padding="same", use_bias=False), keras.layers.BatchNormalization()]
      def call(self, inputs):
          Z = inputs
          for layer in self.main_layers:
              Z = layer(Z)
          skip_Z = inputs
          for layer in self.skip_layers:
              skip_Z = layer(skip_Z)
          return self.activation(Z + skip_Z)
#+end_src
Then in the call() method, we make the inputs go through the main layers and the
skip layers, then we add both outputs and apply the activation function. 

Next, we can build the ResNet-34 using a Sequential model, 
sequence of layers (we can treat each residual unit as a single layer now that
we have the ResidualUnit class): 
#+begin_src python :result outputs

model = keras.models.Sequential()
model.add(keras.layers.Conv2D(64, 7, strides=2, input_shape=[224, 224, 3], padding="same", use_bias=False))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Activation("relu"))
model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding="same"))

prev_filters = 64

for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:
    strides = 1 if filters == prev_filters else 2
    model.add(ResidualUnit(filters, strides=strides))
    prev_filters = filters

model.add(keras.layers.GlobalAvgPool2D())
model.add(keras.layers.Flatten())
model.add(keras.layers.Dense(10, activation="softmax"))

#+end_src

* Using Pretrained Models from Keras
Load the ResNet-50 model, pretrained on ImageNet, 

#+begin_src python :result outputs
model = keras.applications.resnet50.ResNet50(weights="imagenet")
#+end_src

This will create a ResNet-50 model and download weights pretrained on the
ImageNet dataset. 

To use it, you first need to ensure that the images have the right size. 
A ResNet-50 model expects 224 × 224 pixel images, 
so let’s use TensorFlow’s tf.image.resize() function to resize the images
#+begin_src python :result outputs
images_resized = tf.image.resize(images, [224, 224])TIP
#+end_src

The tf.image.resize() will not preserve the aspect ratio. 
So try to cropping the images to the appropriate aspect ratio before
resizing. 
Each model provides a preprocess_input() function that you can use to preprocess
your images. 
#+begin_src python :result outputs
inputs = keras.applications.resnet50.preprocess_input(images_resized * 255)
#+end_src
Now we can use the pretrained model to make predictions:
#+begin_src python :result outputs
Y_proba = model.predict(inputs)
#+end_src
As usual, the output Y_proba is a matrix with one row per image and one
column per class. 

If you want to display the top K predictions, including the class name 
and the estimated probability of each predicted class, 
use the decode_predictions() function. 
#+begin_src python :result outputs

top_K = keras.applications.resnet50.decode_predictions(Y_proba, top=3)
for image_index in range(len(images)):
    print("Image #{}".format(image_index))
for class_id, name, y_proba in top_K[image_index]:
    print(" {} - {:12s} {:.2f}%".format(class_id, name, y_proba * 100))
    print()

# Image #0
#     n03877845 - palace
#     n02825657 - bell_cote
#     n03781244 - monastery42.87%
#     40.57%
#     14.56%
# Image #1
#     n04522168 - vase
#     n07930864 - cup
#     n11939491 - daisy46.83%
#     7.78%
#     4.87%


#+end_src

* Pretrained Models for Transfer Learning
Let’s train a model to classify pictures of flowers, reusing a pretrained
Xception model.

#+begin_src python :result outputs
import tensorflow_datasets as tfds
dataset, info = tfds.load("tf_flowers", as_supervised=True, with_info=True)
dataset_size = info.splits["train"].num_examples # 3670class_names = info.features["label"].names # ["dandelion", "daisy", ...]
n_classes = info.features["label"].num_classes # 5
test_split, valid_split, train_split = tfds.Split.TRAIN.subsplit([10, 15, 75])
test_set = tfds.load("tf_flowers", split=test_split, as_supervised=True)
valid_set = tfds.load("tf_flowers", split=valid_split, as_supervised=True)
train_set = tfds.load("tf_flowers", split=train_split, as_supervised=True)
#+end_src
Next we must preprocess the images. 
The CNN expects 224 × 224 images,
so we need to resize them. 
run the images through Xception’s preprocess_input() function:
#+begin_src python :result outputs
def preprocess(image, label):
    resized_image = tf.image.resize(image, [224, 224])
    final_image =
    keras.applications.xception.preprocess_input(resized_image)
    return final_image, label

batch_size = 32
train_set = train_set.shuffle(1000)
train_set = train_set.map(preprocess).batch(batch_size).prefetch(1)
valid_set = valid_set.map(preprocess).batch(batch_size).prefetch(1)
test_set = test_set.map(preprocess).batch(batch_size).prefetch(1)
#+end_src

To perform some data augmentation, change the preprocessing
function for the training set, adding some random transformations to the
training images. 

 Next let’s load an Xception model, pretrained on ImageNet. 
 We then add our own global average pooling layer, based on the output of the
 base model, 
 followed by a dense output layer with one unit per class, using the
 softmax activation function. 
  Finally, we create the Keras Model:
#+begin_src python  :result outputs
base_model = keras.applications.xception.Xception(weights="imagenet",
include_top=False)
avg = keras.layers.GlobalAveragePooling2D()(base_model.output)
output = keras.layers.Dense(n_classes, activation="softmax")(avg)
model = keras.Model(inputs=base_model.input, outputs=output)
for layer in base_model.layers:
    layer.trainable = FalseNOTE
optimizer = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)
model.compile(loss="sparse_categorical_crossentropy",
optimizer=optimizer,
metrics=["accuracy"])
history = model.fit(train_set, epochs=5, validation_data=valid_set)
#+end_src
After training the model for a few epochs, its validation accuracy should
reach about 75–80% and stop making much progress. 

This means that the top layers are now pretty well trained, so we are ready to
unfreeze all the layers  and continue training
This time we use a much lower learning rate to avoid damaging the
pretrained weights:
#+begin_src python :result outputs

for layer in base_model.layers:
    layer.trainable = True
optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.001)
model.compile(...)
history = model.fit(...)
#+end_src

* Classification and Localization
Localizing an object in a picture can be expressed as a regression task. 
Predict a bounding box around the object, 
Predict the horizontal and vertical coordinates of the object’s center, 
As well as its height and width. 
just need to add a second dense output layer with four units, 

#+begin_src python :result outputs
base_model = keras.applications.xception.Xception(weights="imagenet", include_top=False)
avg = keras.layers.GlobalAveragePooling2D()(base_model.output)
class_output = keras.layers.Dense(n_classes, activation="softmax")(avg)
loc_output = keras.layers.Dense(4)(avg)
model = keras.Model(inputs=base_model.input,outputs=[class_output, loc_output])
model.compile(loss=["sparse_categorical_crossentropy", "mse"],
loss_weights=[0.8, 0.2], # depends on what you care most about
               optimizer=optimizer, metrics=["accuracy"])
#+end_src

the dataset does not have bounding boxes around the flowers. 
So, we need to add them ourselves. 
open source image labeling tools

 - VGG Image Annotator
 - LabelImg
 - OpenLabeler
 - ImgLab
 - commercial tool like LabelBox or Supervisely

*crowdsourcing in computer vision* 

Let's suppose you've obtained the bounding boxes for every image in the
flowers dataset. 

You then need to create a dataset whose items will be batches of
preprocessed images along with their class labels and their bounding
boxes. 

Each item should be a tuple of the form (images, (class_labels, bounding_boxes)). 

The MSE is not a great metric to evaluate how well the model can predict
bounding boxes. 

The most common metric for this is the Intersection over Union (IoU): 
 
* Object Detection
The task of classifying and localizing multiple objects in an image is
called object detection. 

A common approach was to take a CNN that was trained to classify and locate a
single object, then slide it across the image. 
*** non-max suppression 
. Add an extra objectness output to your CNN, to estimate the probability
  that a flower is indeed present in the image. It must use the sigmoid activation function and you can use binary cross-entropy loss. 
  Then get rid of all the bounding boxes for which the objectness score is below some threshold.

- Find the bounding box with the highest objectness score, and get
  rid of all the other bounding boxes that overlap a lot with it. 
  bounding box with the max objectness score is the thick bounding
  box over the topmost rose . 

- Repeat step two until there are no more bounding boxes to get rid of.
This approach works well, but requires running the CNN many times. 
*  Fully Convolutional Networks
Replace the dense layers at the top of a CNN by convolutional layers. 
To convert a dense layer to a convolutional layer, the number of filters in the
convolutional layer must be equal to the number of units in the dense layer, 
the filter size must be equal to the size of the input feature maps, 
and you must use "valid" padding. 
The stride may be set to 1 or more. 
* You Only Look Once (YOLO)
YOLOv3's architecture
- It outputs five bounding boxes for each grid cell, 
  and each bounding box comes with an objectness score. 
  It also outputs 20 class probabilities per grid cell
  That’s a total of 45 numbers per grid cell: 
  5 bounding boxes, each with 4 coordinates, 
  plus 5 objectness scores, 
  plus 20 class probabilities.
- YOLOv3 predicts an offset relative to the coordinates of the grid cell, 
  where (0, 0) means the top left of that cell and (1, 1) 
  means the bottom right. 

  For each grid cell, YOLOv3 is trained to predict only bounding boxes whose
  center lies in that cell.

  YOLOv3 applies the logistic activation function to the bounding
  box coordinates to ensure they remain in the 0 to 1 range.
  
- Before training,  YOLOv3 finds five representative
  bounding box dimensions, called anchor boxes (or bounding box
  priors). It does this by applying the K-Means algorithm to the height and
  width of the training set bounding boxes. 
  for each grid cell and each anchor box, the network predicts the log of the
  vertical and horizontal rescaling factors. 

- The network is trained using images of different scales: every few
  batches during training, the network randomly chooses a new
  image dimension allowing the network to learn to detect objects at different
  scales.

* Semantic Segmentation

In semantic segmentation, each pixel is classified according to the class of
the object it belongs to. 

Different objects of the same class are notdistinguished. 
The main difficulty in this task is that when images go through a regular CNN,
they gradually lose their spatial resolution;
a regular CNN may end up knowing that there’s a person somewhere in
the bottom left of the image, but it will not be much more precise than
that.

Taking a pretrained CNN and turn it into an FCN.

The CNN applies an overall stride of 32 to the input image, 
meaning the last layer outputs feature maps that are 32 times smaller than the
input image. 
add a single upsampling layer that multiplies the resolution by 32. 

There are several solutions available for upsampling, such as bilinear
interpolation, but that only works reasonably well up to ×4 or ×8. 

use a transposed convolutional layer: 
it is equivalent to first stretching the image by inserting empty rows and
columns (full of zeros), then performing a regular convolution. 

Alternatively, some people prefer to think of it as a regularconvolutional 
layer that uses fractional strides.

The transposed convolutional layer can be initialized to perform
something close to linear interpolation, 

but since it is a trainable layer, 

it will learn to do better during training. 

To do better, add skip connections from lower layers: 

In short, 
the output of the original CNN goes through the following extra steps: 

upscale ×2, 
add the output of a lower layer, 
upscale ×2, 
add the output of an even lower layer, and 
finally
upscale ×8. 
It is even possible to scale up beyond the size of the original
image: this can be used to increase the resolution of an image, which is a
technique called super-resolution

Instance segmentation is similar to semantic segmentation, 
but instead of merging all objects of the same class into one
big lump, each object is distinguished from the others. 

