
*  CNNS
IBM’s Deep Blue supercomputer beat the chess world champion Garry Kasparov back in 1996

CNNs emerged from the study of the brain’s visual cortex, and they have been
used in image recognition since the 1980s. 

Due to the increase in computational power, the amount of available training
data, CNNs have managed to achieve superhuman performance on some complex visual tasks. 

They power image search services, 
- Self-driving cars, 
- Automatic video classification
- Systems, 
- Voice recognition 
- Natural language processing. 

* Convolutional Layers

The convolutional layer is the fundamental building block of a CNN, playing a
crucial role in its architecture.

Rather than being connected to every single pixel in the input image, neurons in
the first convolutional layer establish connections solely with pixels within
their respective receptive fields.

In the subsequent convolutional layer, each neuron forms connections exclusively
with neurons positioned in a small rectangular region of the previous layer.

This design enables the network to focus on extracting small-scale, low-level
features in the initial hidden layer, which are then progressively combined to
form larger, high-level features in subsequent layers.

One notable characteristic of CNNs is that each layer is represented in a 2D
format, facilitating the correspondence between neurons and their corresponding
inputs.

To illustrate, a neuron situated at row 'i' and column 'j' in a given layer
establishes connections with the outputs of neurons in the previous layer,
specifically within the range of:

| Position of a neuron | Connections in the previous layer |
| (i,j)                | (i:i+f_h-1, j:j+f_w-1)              |

Here, 'f_h' and 'f_w' denote the height and width of the receptive field, respectively.
** *Padding*
In order for a layer to have the same height and width as the previous layer, 
it is common to add zeros around the inputs. This is called zero padding.

** *Strides*
It is also possible to connect a large input layer to a much smaller layer by
spacing out the receptive fields, as shown in Figure 14-4. This
dramatically reduces the model’s computational complexity. The shift
from one receptive field to the next is called the stride. 

| Position of a neuron | Connections in the previous layer |
| (i,j)                | (i*s_h:i*s_h+f_h-1,j:j*s_w+f_w-1)      |
where s_h and s_w are the vertical and horizontal strides.

** Filters (Convolutional Kernel) 
A neuron's weights can be represented as a small image the size of the
receptive field, called *filters* or *convolution kernels*. 
Thus, a layer full of neurons using the same filter outputs a *feature map*, 
which highlights the areas in an image that activate the filter the most. 

You do not have to define the filters manually. 
During training the convolutional layer will automatically learn the most useful
filters for its task, and the layers above will learn to combine them into more
complex patterns.

In reality a convolutional layer has multiple filters and outputs 
*one feature map per filter*. 

It has one neuron per pixel in each feature map, 

All neurons within a given feature map share the same parameters. 

A neuron’s receptive field is the same as described earlier, but it extends
across all the previous layers’ feature maps. 

A convolutional layer simultaneously applies multiple trainable
filters to its inputs, making it capable of detecting multiple features anywhere
in its inputs.

The fact that all neurons in a feature map share the same parameters dramatically
reduces the number of parameters in the model. 

Once the CNN has learned to recognize a pattern in one location, it can
recognize it in any other location.

|------------------------+----------------------------------|
| layer l, feature map k | layer l-1                        |
|------------------------+----------------------------------|
| (i,j)                  | (ixs_h:ixs_h+f_h-1, jxs_w:jxs_w+f_w-1) |
|------------------------+----------------------------------|

* Memory Requirements
During the reverse pass of backpropagation, it is necessary to have access to
all the intermediate values that were calculated during the forward pass.

In the event of a training failure caused by an out-of-memory error, there are
several approaches you can take to address the issue. One option is to reduce
the size of the mini-batches used in training. By decreasing the number of
examples processed in each iteration, you can mitigate memory constraints.

Another strategy is to reduce the dimensionality of the network. This can be
achieved by employing a stride, which decreases the spatial resolution of
feature maps, or by removing a few layers from the architecture. Both methods
help reduce the memory requirements.

Switching from 32-bit floating-point precision to 16-bit floating-point
precision is another potential solution. This reduces the memory footprint of
the network at the cost of some loss in numerical precision.

Alternatively, if available, you can distribute the CNN across multiple
devices. This approach allows you to utilize the memory resources of multiple
hardware devices in parallel, alleviating the memory limitations.

By employing these techniques, you can tackle out-of-memory errors during
training and continue the optimization process of your CNN.
* Pooling Layers
Each neuron within a pooling layer establishes connections exclusively with a
limited number of neurons from the previous layer. These connections are formed
within a small rectangular receptive field, the dimensions of which are
determined by the size, stride, and padding type used in the pooling
operation. It's important to note that pooling layers do not involve weights.

In addition to its connectivity pattern, a max pooling layer introduces a degree
of invariance to small translations. This means that the layer's output remains
relatively unchanged even if the input undergoes slight shifts in position.

By incorporating max pooling layers at regular intervals within a CNN
architecture, it becomes possible to achieve a certain level of translation
invariance on a larger scale. This means that the network becomes less sensitive
to small positional variations of features.

Moreover, max pooling also offers a limited amount of rotational invariance and
slight scale invariance. This implies that the network's predictions are less
affected by moderate rotations or variations in the size of the input.

These types of invariances provided by max pooling can be advantageous in
scenarios where the prediction should not rely heavily on precise details, such
as in classification tasks. They allow the network to focus more on the overall
features and patterns rather than being overly influenced by specific positional
or scaling factors.

* TensorFlow Implementation

By default, the stride in max pooling layers is set to the size of the pooling
kernel. For example, in Keras, you can create a max pooling layer with a pool
size of 2 using the following code:
#+begin_src  python
max_pool = keras.layers.MaxPool2D(pool_size=2)
#+end_src  

The depthwise max pooling layer is not available in Keras, but TensorFlow's
low-level Deep Learning API does include this functionality. For instance, you
can use the tf.nn.max_pool function to perform depthwise max pooling on
images. The following code snippet demonstrates its usage:

#+begin_src  python
output = tf.nn.max_pool(
                        images,
                        ksize=(1, 1, 1, 3),
                        strides=(1, 1, 1, 3),
                        padding="valid"
)
#+end_src  python


You can include this as a layer in your Keras models

#+begin_src  python
depth_pool = keras.layers.Lambda(
                  lambda X: tf.nn.max_pool(
                                           X, 
                                           ksize=(1, 1, 1, 3), 
                                           strides=(1, 1, 1, 3),
                                           padding="valid"
))
#+end_src  python

Global average pooling layer. 

#+begin_src  python
global_avg_pool = keras.layers.GlobalAvgPool2D()
#+end_src  python
It’s equivalent to this simple Lambda layer, which computes the mean over
the spatial dimensions (height and width):

#+begin_src  python
global_avg_pool = keras.layers.Lambda(lambda X: tf.reduce_mean(X, axis= [1, 2]))
#+end_src  python

* Tensor Flow Implementation 
Input image: [height, width, channels]. 
A mini-batch: [mini-batch size, height, width, channels]. 
Weights of convolutional layer: [f_h, f_w, f_n′, f_n].
The bias terms of a convolutional layer:  1D tensor of shape [f_n].

#+begin_src python   :results output
from sklearn.datasets import load_sample_image
china = load_sample_image("china.jpg") / 255
flower = load_sample_image("flower.jpg") / 255
images = np.array([china, flower])
batch_size, height, width, channels = images.shape
filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)
filters[:, 3, :, 0] = 1 # vertical line
filters[3, :, :, 1] = 1 # horizontal line
outputs = tf.nn.conv2d(images, filters, strides=1, padding="same")
plt.imshow(outputs[0, :, :, 1], cmap="gray") # plot 1st image's 2nd
feature map
plt.show()
#+end_src 

Strides is equal to 1, 
Strides could could also be a 1D array with four elements, 
where the two central elements are the vertical and horizontal strides 
(s_h and s_w). 
The first and last elements must currently be equal to 1.

If set to "valid", the convolutional layer does not use zero padding and may
ignore some rows and columns at the bottom and right of the input image,
depending on the stride. This means that every neuron’s receptive field lies
strictly within valid positions inside the input, hence the name valid.

In a real CNN you would normally define filters as trainable variables so 
the neural net can learn which filters work best.

#+begin_src python 
conv = keras.layers.Conv2D(filters=32, 
                           kernel_size=3, 
                           strides=1,
                           padding="same", 
                           activation="relu")
#+end_src 
* CNN Architectures
Typical CNN architectures stack a 
few convolutional layers (each one generally followed by a ReLU layer), 
then a pooling layer, 
then another few convolutional layers (+ReLU), 
then another pooling layer, 
and so on.

The image gets smaller and smaller as it progresses through the network,
but it also typically gets deeper and deeper. 

At the top of the stack, a regular feedforward neural network is added, 
composed of a few fully connected layers (+ReLUs), 

and the final layer outputs the prediction.

A common mistake is to use convolution kernels that are too large. 
Smaller kernels use fewer parameters and require fewer computations, and it will
usually perform better. One exception is for the first convolutional layer

#+begin_src python 
model = keras.models.Sequential([
keras.layers.Conv2D(64, 7, activation="relu", padding="same", input_shape=[28, 28, 1]),
keras.layers.MaxPooling2D(2),
keras.layers.Conv2D(128, 3, activation="relu", padding="same"),
keras.layers.Conv2D(128, 3, activation="relu", padding="same"),
keras.layers.MaxPooling2D(2),
keras.layers.Conv2D(256, 3, activation="relu", padding="same"),
keras.layers.Conv2D(256, 3, activation="relu", padding="same"),
keras.layers.MaxPooling2D(2),
keras.layers.Flatten(),
keras.layers.Dense(128, activation="relu"),
keras.layers.Dropout(0.5),
keras.layers.Dense(64, activation="relu"),
keras.layers.Dropout(0.5),
keras.layers.Dense(10, activation="softmax")])
#+end_src

This CNN reaches over 92% accuracy on the test set. 
* LeNet-5
It was created by Yann LeCun in 1998 and has been widely used for handwritten
digit recognition (MNIST). 
* AlexNet

The AlexNet CNN architecture, developed by Alex Krizhevsky et al., achieved
significant success in the 2012 ImageNet ILSVRC challenge, with a top-five error
rate of 17%. One of the notable aspects of AlexNet was its introduction of
stacking convolutional layers directly on top of one another, which was a
departure from previous approaches.

To address the issue of overfitting, the authors incorporated two regularization
techniques in AlexNet.

The first technique employed was dropout, where during training, randomly
selected neurons in a layer are "dropped out" or temporarily ignored. In
AlexNet, a dropout rate of 50% was applied, meaning that each neuron had a 50%
chance of being dropped out during training. Dropout helps prevent co-adaptation
of neurons, forcing the network to rely on more robust and general features.

Additionally, AlexNet utilized a technique called local response normalization
(LRN) immediately after the Rectified Linear Unit (ReLU) activation step in
layers C1 and C3. LRN involves inhibiting neurons in a feature map that are
strongly activated and located at the same position as neurons in neighboring
feature maps. This competition between neurons encourages specialization and
diversification among feature maps, leading to a broader exploration of features
and improved generalization capabilities.

By incorporating dropout regularization and local response normalization,
AlexNet aimed to mitigate overfitting and enhance the network's ability to
generalize well to unseen data. These techniques played a crucial role in the
architecture's success in the ImageNet challenge.

*  GoogLeNet 

The GoogLeNet CNN architecture, developed by Christian Szegedy et al. from
Google Research, achieved remarkable success in the ILSVRC 2014 challenge with a
top-five error rate below 7%. It was designed to be much deeper than previous
CNNs and introduced the concept of inception modules, which enabled the network
to use parameters more efficiently.

An inception module in GoogLeNet consists of multiple parallel branches, each
performing a different type of convolutional operation. For example, a notation
like "3 × 3 + 1(S)" indicates a branch that uses a 3 × 3 kernel, stride 1, and
"same" padding. The input signal is first copied and fed into each branch
independently. All convolutional layers in the inception module use the
ReLU activation function.

The outputs from all branches are then concatenated along the depth dimension in
the final depth concatenation layer. The inception modules serve multiple
purposes. While they cannot capture spatial patterns, they excel at capturing
patterns along the depth dimension. Additionally, by outputting fewer feature
maps than their inputs, they act as bottleneck layers, reducing
dimensionality. Moreover, each pair of convolutional layers within an inception
module acts as a powerful convolutional layer, capable of capturing more complex
patterns. In essence, these pairs of convolutional layers sweep a two-layer
neural network across the image, akin to a simple linear classifier.

The number of convolutional kernels for each convolutional layer in the
inception module is a hyperparameter that can be adjusted. GoogLeNet consists of
nine inception modules, and the six numbers associated with each module
represent the number of feature maps output by each convolutional layer within
the module.

The initial layers of GoogLeNet divide the image's height and width by 4,
effectively reducing its area by 16. Subsequently, a local response
normalization layer ensures that the preceding layers learn a diverse range of
features. Following this, two convolutional layers are employed, where the first
acts as a bottleneck layer.

Due to the dimensionality reduction achieved by the bottleneck layer, it is
unnecessary to have multiple fully connected layers at the top of the CNN. This
significantly reduces the number of parameters in the network and mitigates the
risk of overfitting.

* VGGNet
It had a very simple and classical architecture, 
with 2 or 3 convolutional layers and a pooling layer, 
then again 2 or 3 convolutional layers and a pooling layer, 
and so on, 
plus a final dense network with 2 hidden layers and the output layer. 
It used only 3 × 3 filters, but many filters.
* ResNet

ResNet (short for Residual Network) is a type of deep convolutional neural
network that addresses the challenges of training very deep models with an
increasing number of parameters. It introduces skip connections, also known as
residual connections, to facilitate the learning process.

As models become deeper, they can encounter difficulties in convergence and
information flow. ResNet aims to overcome these challenges by learning residual
functions to model the target function, denoted as h(x), where x represents the
input. By adding the input x to the output of the network, ResNet forces the
network to learn the residual function f(x) = h(x) - x. This approach proves
beneficial when the target function is similar to the identity function, as it
speeds up training considerably.

The addition of skip connections allows the network to make progress even if
some layers are not yet effectively learning. These skip connections enable the
signal to propagate easily across the entire network, facilitating gradient flow
and reducing the vanishing gradient problem.

ResNet can be visualized as a stack of residual units, where each unit comprises
two convolutional layers. The number of feature maps is typically doubled every
few residual units, while their height and width are halved. However, due to the
change in dimensions, the input cannot be directly added to the output of the
residual unit. To address this issue, ResNet utilizes a 1 × 1 convolutional
layer with a stride of 2 and the appropriate number of output feature maps. This
step ensures that the input is transformed to match the dimensions of the output
before being added to the residual unit's output.

By incorporating skip connections and residual units, ResNet effectively trains
deeper models with improved information flow and faster convergence. It has
achieved impressive results in various computer vision tasks and has become a
popular architecture in the deep learning community.

* Xception

The Xception architecture merges ideas from both the GoogLeNet and ResNet
architectures but replaces the inception modules with a specialized layer called
a depthwise separable convolution layer. While separable convolution layers were
used in some earlier CNN architectures, they take on a central role in the
Xception architecture.

A depthwise separable convolution layer makes the assumption that spatial
patterns and cross-channel patterns can be modeled independently. It consists of
two parts. The first part involves applying a single spatial filter to each
input feature map, focusing on capturing spatial patterns. The second part
focuses on identifying cross-channel patterns, which capture relationships
between different channels of the input.

In the Xception architecture, the initial layers comprise regular convolutional
layers. However, for the remainder of the architecture, separable convolutions
are predominantly used, along with a few max pooling layers and the usual final
layers (e.g., fully connected layers).

One of the advantages of separable convolutional layers is that they require
fewer parameters, consume less memory, and involve fewer computations compared
to regular convolutional layers. Additionally, they often yield better
performance. As a result, it is recommended to consider using separable
convolutions as the default choice in CNN architectures.

The Xception architecture leverages depthwise separable convolution layers to
effectively capture spatial and cross-channel patterns while being
computationally efficient and parameter-efficient. This approach has
demonstrated impressive results in various computer vision tasks.

* SENet

The extended versions of Inception networks and ResNets, namely SE-Inception and
SE-ResNet, incorporate an additional neural network component called an SE block
into each unit of the original architectures.

The SE block is responsible for analyzing the output of the unit it is attached
to, focusing exclusively on the depth dimension (the channels), without
considering spatial patterns. Its purpose is to learn which features tend to be
active together. By leveraging this learned information, the SE block
recalibrates the feature maps to enhance their effectiveness.

For example, an SE block might learn that in images, mouths, noses, and eyes
typically appear together. If it observes strong activation in the mouth and
nose feature maps but only mild activation in the eye feature map, it will boost
the eye feature map (or, more accurately, reduce irrelevant feature maps). This
recalibration process helps resolve ambiguities and improve the overall
representation.

An SE block consists of three layers: a global average pooling layer, a hidden
dense layer with the ReLU activation function, and a dense output layer with the
sigmoid activation function. The global average pooling layer computes the mean
activation for each feature map, compressing the responses into a small vector
representation. This low-dimensional vector serves as an embedding of the
distribution of feature responses, capturing the combinations of features. The
output layer takes this embedding and generates a recalibration vector, with one
number per feature map, ranging from 0 to 1. The feature maps are then
multiplied by this recalibration vector, scaling down irrelevant features (with
low recalibration scores) while preserving relevant features (with recalibration
scores close to 1).

By incorporating SE blocks into the architecture, SE-Inception and SE-ResNet
models gain a boost in performance. The SE blocks allow the models to
dynamically recalibrate feature maps based on their interdependencies, improving
the representation power and enhancing the network's ability to capture complex
patterns and relationships.

* 
