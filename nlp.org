
* Natural Language Processing with RNNs and Attention
A common approach for natural language tasks is to use recurrent neural
networks. 
* Generating Shakespearean Text Using a Character RNN
This Char-RNN can be used to generate novel text, one character at a time. 
Here is a small sample of the text generated by a Char-RNN model after it was
trained on all of Shakespeare's work:
#+begin_example
Alas, I think he shall be come approached and the day
When little srain would be attain’d into being never fed,
And who is but a chain and subjects of his death,
I should not sleep.
#+end_example
* Creating the Training Dataset
First, let's download all of Shakespeare's work, using Keras's handy
get_file() function and downloading the data from Andrej Karpathy's
Char-RNN project:
#+begin_src python :result outputs
shakespeare_url = "https://homl.info/shakespeare" # shortcut URL
filepath = keras.utils.get_file("shakespeare.txt", shakespeare_url)
with open(filepath) as f:
    shakespeare_text = f.read()
#+end_src
Next, we must encode every character as an integer. 
But in this case, it will be simpler to use Keras's Tokenizer class.
First we need to fit a tokenizer to the text: 
it will find all the characters used in the text and map
each of them to a different character ID, from 1 to the number of distinct
characters:
#+begin_src python :result outputs
tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)
tokenizer.fit_on_texts([shakespeare_text])
#+end_src
We set char_level=True to get character-level encoding rather than the
default word-level encoding. 
Note that this tokenizer converts the text to
lowercase by default. 

Now the tokenizer can encode a sentence to a
list of character IDs and back, and it tells us how many distinct characters
there are and the total number of characters in the text:

#+begin_src python :result outputs
tokenizer.texts_to_sequences(["First"]) #[[20, 6, 9, 8, 3]]
tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]]) # ['f i r s t']
max_id = len(tokenizer.word_index) # number of distinct characters
dataset_size = tokenizer.document_count # total number of characters
#+end_src
Let's encode the full text so each character is represented by its ID 
and subtract 1 to get IDs from 0 to 38, rather than from 1 to 39:
#+begin_src python :result outputs
[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1
#+end_src
* How to Split a Sequential Dataset
It is very important to avoid any overlap between the training set, the
validation set, and the test set. 
It is often safer to split across time—but this implicitly assumes that
the patterns the RNN can learn in the past will still
exist in the future. In other words, we assume that the time series is
stationary. 
For many time series this assumption is reasonable, but for many others it is
not. To make
sure the time series is indeed sufficiently stationary, you can plot the
model's errors on the validation set across time: if the model performs
much better on the first part of the validation set than on the last part, then
the time series may not be stationary enough, and you might be better off
training the model on a shorter time span.

Let's take the first 90% of the text for the
training set, and
create a tf.data.Dataset that will return each character one by one from
this set:
#+begin_src python :result outputs
train_size = dataset_size * 90 // 100
dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])
#+end_src
* Chopping the Sequential Dataset into Multiple Windows
we will use
the dataset's window() method to convert this long sequence of characters
into many smaller windows of text. Every instance in the dataset will be a
fairly short substring of the whole text, and the RNN will be unrolled only
over the length of these substrings. This is called 
*truncated backpropagation through time*. 

#+begin_src python :result outputs
n_steps = 100
window_length = n_steps + 1 # target = input shifted 1 character ahead
dataset = dataset.window(window_length, shift=1, drop_remainder=True)
#+end_src

By default, the window() method creates nonoverlapping windows, but to
get the largest possible training set we use shift=1 so that the first
window contains characters 0 to 100, the second contains characters 1 to
101, and so on. To ensure that all windows are exactly 101 characters long
we set drop_remainder=True.

The window() method creates a dataset that contains windows, each of
which is also represented as a dataset. It's a nested dataset, analogous to a
list of lists.  We must call the flat_map()
method to convert a nested dataset into a flat dataset. 
The flat_map() method takes a function as an argument, which
allows you to transform each dataset in the nested dataset before
flattening. 
ds.batch(2) to flat_map(), then it will transform the nested dataset {{1,
2}, {3, 4, 5, 6}} into the flat dataset {[1, 2], [3, 4], [5, 6]}: it’s a dataset of
tensors of size 2. With that in mind, we are ready to flatten our dataset:
dataset = dataset.flat_map(lambda window: window.batch(window_length))
Notice that we call batch(window_length) on each window: since all
windows have exactly that length, we will get a single tensor for each of
them. Now the dataset contains consecutive windows of 101 characters
each. Since Gradient Descent works best when the instances in the training
set are independent and identically distributed (see Chapter 4), we need to
shuffle these windows. Then we can batch the windows and separate the
inputs (the first 100 characters) from the target (the last character):
batch_size = 32
dataset = dataset.shuffle(10000).batch(batch_size)
dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))
Figure 16-1 summarizes the dataset preparation steps discussed so far
(showing windows of length 11 rather than 101, and a batch size of 3
instead of 32).Figure 16-1. Preparing a dataset of shuffled windows
As discussed in Chapter 13, categorical input features should generally be
encoded, usually as one-hot vectors or as embeddings. Here, we will
encode each character using a one-hot vector because there are fairly few
distinct characters (only 39):
dataset = dataset.map(
lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id),
Y_batch))
Finally, we just need to add prefetching:
dataset = dataset.prefetch(1)
That’s it! Preparing the dataset was the hardest part. Now let’s create the
model.
Building and Training the Char-RNN Model
To predict the next character based on the previous 100 characters, we can
use an RNN with 2 GRU layers of 128 units each and 20% dropout on both
the inputs (dropout) and the hidden states (recurrent_dropout). We can
tweak these hyperparameters later, if needed. The output layer is a time-
distributed Dense layer like we saw in Chapter 15. This time this layermust have 39 units (max_id) because there are 39 distinct characters in the
text, and we want to output a probability for each possible character (at
each time step). The output probabilities should sum up to 1 at each time
step, so we apply the softmax activation function to the outputs of the
Dense layer. We can then compile this model, using the
"sparse_categorical_crossentropy" loss and an Adam optimizer.
Finally, we are ready to train the model for several epochs (this may take
many hours, depending on your hardware):
model = keras.models.Sequential([
keras.layers.GRU(128, return_sequences=True, input_shape=[None,
max_id],
dropout=0.2, recurrent_dropout=0.2),
keras.layers.GRU(128, return_sequences=True,
dropout=0.2, recurrent_dropout=0.2),
keras.layers.TimeDistributed(keras.layers.Dense(max_id,
activation="softmax"))
])
model.compile(loss="sparse_categorical_crossentropy", optimizer="adam")
history = model.fit(dataset, epochs=20)
Using the Char-RNN Model
Now we have a model that can predict the next character in text written by
Shakespeare. To feed it some text, we first need to preprocess it like we
did earlier, so let’s create a little function for this:
def preprocess(texts):
X = np.array(tokenizer.texts_to_sequences(texts)) - 1
return tf.one_hot(X, max_id)
Now let’s use the model to predict the next letter in some text:
>>> X_new = preprocess(["How are yo"])
>>> Y_pred = model.predict_classes(X_new)
>>> tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last
char
'u'Success! The model guessed right. Now let’s use this model to generate
new text.
Generating Fake Shakespearean Text
To generate new text using the Char-RNN model, we could feed it some
text, make the model predict the most likely next letter, add it at the end of
the text, then give the extended text to the model to guess the next letter,
and so on. But in practice this often leads to the same words being
repeated over and over again. Instead, we can pick the next character
randomly, with a probability equal to the estimated probability, using
TensorFlow’s tf.random.categorical() function. This will generate
more diverse and interesting text. The categorical() function samples
random class indices, given the class log probabilities (logits). To have
more control over the diversity of the generated text, we can divide the
logits by a number called the temperature, which we can tweak as we
wish: a temperature close to 0 will favor the high-probability characters,
while a very high temperature will give all characters an equal probability.
The following next_char() function uses this approach to pick the next
character to add to the input text:
def next_char(text, temperature=1):
X_new = preprocess([text])
y_proba = model.predict(X_new)[0, -1:, :]
rescaled_logits = tf.math.log(y_proba) / temperature
char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1
return tokenizer.sequences_to_texts(char_id.numpy())[0]
Next, we can write a small function that will repeatedly call next_char()
to get the next character and append it to the given text:
def complete_text(text, n_chars=50, temperature=1):
for _ in range(n_chars):
text += next_char(text, temperature)
return textWe are now ready to generate some text! Let’s try with different
temperatures:
>>> print(complete_text("t", temperature=0.2))
the belly the great and who shall be the belly the
>>> print(complete_text("w", temperature=1))
thing? or why you gremio.
who make which the first
>>> print(complete_text("w", temperature=2))
th no cce:
yeolg-hormer firi. a play asks.
fol rusb
Apparently our Shakespeare model works best at a temperature close to 1.
To generate more convincing text, you could try using more GRU layers and
more neurons per layer, train for longer, and add some regularization (for
example, you could set recurrent_dropout=0.3 in the GRU layers).
Moreover, the model is currently incapable of learning patterns longer
than n_steps, which is just 100 characters. You could try making this
window larger, but it will also make training harder, and even LSTM and
GRU cells cannot handle very long sequences. Alternatively, you could use
a stateful RNN.
Stateful RNN
Until now, we have used only stateless RNNs: at each training iteration the
model starts with a hidden state full of zeros, then it updates this state at
each time step, and after the last time step, it throws it away, as it is not
needed anymore. What if we told the RNN to preserve this final state after
processing one training batch and use it as the initial state for the next
training batch? This way the model can learn long-term patterns despite
only backpropagating through short sequences. This is called a stateful
RNN. Let’s see how to build one.
First, note that a stateful RNN only makes sense if each input sequence in
a batch starts exactly where the corresponding sequence in the previous
batch left off. So the first thing we need to do to build a stateful RNN is touse sequential and nonoverlapping input sequences (rather than the
shuffled and overlapping sequences we used to train stateless RNNs).
When creating the Dataset, we must therefore use shift=n_steps
(instead of shift=1) when calling the window() method. Moreover, we
must obviously not call the shuffle() method. Unfortunately, batching is
much harder when preparing a dataset for a stateful RNN than it is for a
stateless RNN. Indeed, if we were to call batch(32), then 32 consecutive
windows would be put in the same batch, and the following batch would
not continue each of these window where it left off. The first batch would
contain windows 1 to 32 and the second batch would contain windows 33
to 64, so if you consider, say, the first window of each batch (i.e., windows
1 and 33), you can see that they are not consecutive. The simplest solution
to this problem is to just use “batches” containing a single window:
dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])
dataset = dataset.window(window_length, shift=n_steps,
drop_remainder=True)
dataset = dataset.flat_map(lambda window: window.batch(window_length))
dataset = dataset.batch(1)
dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))
dataset = dataset.map(
lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id),
Y_batch))
dataset = dataset.prefetch(1)
Figure 16-2 summarizes the first steps.Figure 16-2. Preparing a dataset of consecutive sequence fragments for a stateful RNN
Batching is harder, but it is not impossible. For example, we could chop
Shakespeare’s text into 32 texts of equal length, create one dataset of
consecutive input sequences for each of them, and finally use
tf.train.Dataset.zip(datasets).map(lambda *windows:
tf.stack(windows)) to create proper consecutive batches, where the nth
input sequence in a batch starts off exactly where the nth input sequence
ended in the previous batch (see the notebook for the full code).
Now let’s create the stateful RNN. First, we need to set stateful=True
when creating every recurrent layer. Second, the stateful RNN needs to
know the batch size (since it will preserve a state for each input sequence
in the batch), so we must set the batch_input_shape argument in the first
layer. Note that we can leave the second dimension unspecified, since the
inputs could have any length:
model = keras.models.Sequential([
keras.layers.GRU(128, return_sequences=True, stateful=True,
dropout=0.2, recurrent_dropout=0.2,
batch_input_shape=[batch_size, None, max_id]),
keras.layers.GRU(128, return_sequences=True, stateful=True,
dropout=0.2, recurrent_dropout=0.2),
keras.layers.TimeDistributed(keras.layers.Dense(max_id,
activation="softmax"))
])At the end of each epoch, we need to reset the states before we go back to
the beginning of the text. For this, we can use a small callback:
class ResetStatesCallback(keras.callbacks.Callback):
def on_epoch_begin(self, epoch, logs):
self.model.reset_states()
And now we can compile and fit the model (for more epochs, because each
epoch is much shorter than earlier, and there is only one instance per
batch):
model.compile(loss="sparse_categorical_crossentropy", optimizer="adam")
model.fit(dataset, epochs=50, callbacks=[ResetStatesCallback()])
TIP
After this model is trained, it will only be possible to use it to make predictions for
batches of the same size as were used during training. To avoid this restriction, create
an identical stateless model, and copy the stateful model’s weights to this model.
Now that we have built a character-level model, it’s time to look at word-
level models and tackle a common natural language processing task:
sentiment analysis. In the process we will learn how to handle sequences
of variable lengths using masking.

* 
