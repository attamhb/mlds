
* Generating Shakespearean Text Using a Character RNN
- The Unreasonable Effectiveness of Recurrent Neural Networks, by Andrej Karpathy, 2015 
- Train an RNN to predict the next character in a sentence. 
* Creating the Training Dataset
- Download all of Shakespeare's work
- Fit a tokenizer to the text
- The tokenizer converts the text to lowercase by default. 
- 0 is used for masking
- Encode every character as an integer. 

#+begin_src python :result outputs

 filepath = keras.utils.get_file("shakespeare.txt", shakespeare_url)
 with open(filepath) as f:
     shakespeare_text = f.read()

# set char_level=True to get character-level encoding 
tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)
tokenizer.fit_on_texts([shakespeare_text])

tokenizer.texts_to_sequences(["First"]) # [[20, 6, 9, 8, 3]]
tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]]) # ['f i r s t']

max_id = len(tokenizer.word_index) # number of distinct characters
dataset_size = tokenizer.document_count # total number of characters

#subtract 1 to get IDs from 0 to 38, rather than from 1 to 39):
[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1
#+end_src

* How to Split a Sequential Dataset
- Avoid overlap between the training, validation and test set. 
- Leave gap between these sets to avoid the risk of a paragraph overlapping 
- Often safer to split across time—but this implicitly assumes that the patterns the RNN can learn in the past will still exist in the future. 
- Stationary time series (e.g., chemical reactions, law's of chemistry) 
- To make sure the time series is indeed sufficiently stationary, you can plot the model's errors on the validation set across time
- If the model performs much better on the first part of the validation set than on the last part, then the time series may not be stationary enough.
- create a tf.data.Dataset that will return each character one by one from this set: 

  #+begin_src python :result outputs
  train_size = dataset_size * 90 // 100 
  dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size]) 
  #+end_src

* Chopping the Sequential Dataset into Multiple Windows
- The training set consists of a single sequence of over a million characters.
- Convert this long sequence of characters into many smaller windows of text.
- The RNN will be unrolled only over the length of these sub-strings.
- This is called truncated back-propagation through time.
- By default, the window() method creates nonoverlapping windows.
- The window() method creates a nested dataset.
- Can shuffle them or batch them.
- Cannot use a nested dataset directly for training
- Model expects tensors as input, not datasets.
- Call the flat_map() method to convert a nested dataset into a flat dataset.
- The flat_map() method takes a function as an argument.
- Shuffle the windows (Gradient Descent works best) 
- Batch the windows and separate the inputs (the first 100 characters) from the target (the last character)
- Encode each character using a one-hot vector  (only 39 distinct characters)
- Add prefetching. 

#+begin_src  python :result outputs
wsteps = 100 window_length = n_steps + 1 # target = input shifted 1 character ahead 
dataset = dataset.window(window_length, shift=1, drop_remainder=True)
dataset = dataset.flat_map(lambda window: window.batch(window_length))
batch_size = 32 
dataset = dataset.shuffle(10000).batch(batch_size) 
dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))
dataset = dataset.map(
lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))
dataset = dataset.prefetch(1)
#+end_src 

* Building and Training the Char-RNN Model
- Use an RNN with 2 GRU layers
- Each layer with 128 units and 20% dropout on both the inputs (dropout) and the hidden states (recurrent_dropout).
- May tweak these hyperparameters later.
- The output layer is a time-distributed Dense layer.
- This time this layer must have 39 units
- Output a probability for each possible character (at each time step).
- The output probabilities should sum up to 1 at each time step.

- Apply the softmax activation function to the outputs of the Dense layer.

- Compile this model, using the "sparse_categorical_crossentropy" loss and an Adam optimizer.

- Train the model for several epochs 

#+begin_src python :result outputs
model = keras.models.Sequential(
[
keras.layers.GRU(
128, return_sequences=True, input_shape=[None, max_id], dropout=0.2, recurrent_dropout=0.2
), 
keras.layers.GRU(
128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2
), 
keras.layers.TimeDistributed(
keras.layers.Dense(max_id, activation="softmax")
)
]
) 

model.compile(loss="sparse_categorical_crossentropy", optimizer="adam")

history = model.fit(dataset, epochs=20)


  

#+end_src
* Using the Char-RNN Model
- Create a function to preprocess text.
- Use the model to predict the next letter in some text
#+begin_src python :result outputs

def preprocess(texts):
    X = np.array(tokenizer.texts_to_sequences(texts)) - 1
    return tf.one_hot(X, max_id)

X_new = preprocess(["How are yo"])
Y_pred = model.predict_classes(X_new)

tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] 
# 1st sentence, last char 'u'

#+end_src
* Generating Fake Shakespearean Text

- Pick the next character randomly, with a probability equal to the estimated probability

- The categorical() function samples random class indices, given the class log probabilities (logits).

- To have more control over the diversity of the generated text, divide the logits by a number called the temperature

- A temperature close to 0 will favor the high-probability characters.

- A very high temperature will give all characters an equal probability.

- Generate some text with different temperatures

- Shakespeare model may work best at a temperature close to 1
  
- To generate more convincing text, use more GRU layers and
  more neurons per layer, train for longer, and add some regularization.
  
- The model is incapable of learning patterns longer than n_steps, 

- Try making this window larger, using a stateful RNN.

#+begin_src python :result outputs

def next_char(text, temperature=1):
    X_new = preprocess([text])
    y_proba = model.predict(X_new)[0, -1:, :]
    rescaled_logits = tf.math.log(y_proba) / temperature
    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1
    return tokenizer.sequences_to_texts(char_id.numpy())[0]

def complete_text(text, n_chars=50, temperature=1):
    for _ in range(n_chars):
    text += next_char(text, temperature)
    return text

print(complete_text("t", temperature=0.2))
# the belly the great and who shall be the belly the
print(complete_text("w", temperature=1))
# thing? or why you gremio.
# who make which the first
print(complete_text("w", temperature=2))
# th no cce:
# yeolg-hormer firi. a play asks.
# fol rusb

#+end_src

* Stateful RNN

- Stateless RNNs: at each training iteration the model starts with a hidden state full of zeros, then it updates this state at each time step, and after the last time step, it throws it away, as it is not needed anymore. 

- Stateful RNNs: Preserve the final state after processing one training batch and use it as the initial state for the next training batch. 

- This way the model can learn long-term patterns despite only backpropagating through short sequences. 

- Stateful RNN only makes sense if each input sequence in a batch starts exactly where the corresponding sequence in the previous
  batch left off. 

- Use sequential and nonoverlapping input sequences rather than the shuffled and overlapping sequences.
 
- Use shift=n_steps instead of shift=1

- Do not call the shuffle() method. 

- Batching is harder when preparing a dataset for a stateful RNN.

- If called batch(32), then 32 consecutive windows would be put in the same batch, and the following batch would not continue each of these window where it left off.

- The first batch would contain windows 1 to 32 and the second batch would contain windows 33 to 64.

- If you consider, say, the first window of each batch (i.e., windows 1 and 33), you can see that they are not consecutive. 

- Just use "batches" containing a single window to overcome this issue??

- Batching is harder, but it is not impossible. 

- chop Shakespeare’s text into 32 texts of equal length, create one dataset of consecutive input sequences for each of them.

- use tf.train.Dataset.zip(datasets).map(lambda *windows: tf.stack(windows)) to create proper consecutive batches, 

- the nth input sequence in a batch starts off exactly where the nth input sequence ended in the previous batch.

- create the stateful RNN. 

- set stateful=True

- the stateful RNN needs to know the batch size, so we must set the batch_input_shape argument in the first layer. 

- leave the second dimension unspecified, since the inputs could have any length:

- At the end of each epoch, reset the states before going back to the beginning of the text. 

- compile and fit the model

- After training, only possible to use it to make predictions for batches of the same size as during training. 

- alternatively create an identical stateless model, and copy the stateful model’s weights to this model.



#+begin_src python :result outputs

dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])
dataset = dataset.window(window_length, shift=n_steps,
drop_remainder=True)

dataset = dataset.flat_map(lambda window: window.batch(window_length))
dataset = dataset.batch(1)
dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))


dataset = dataset.map(
lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id),
Y_batch))


dataset = dataset.prefetch(1)

model = keras.models.Sequential([
keras.layers.GRU(128, return_sequences=True, stateful=True,
dropout=0.2, recurrent_dropout=0.2,
batch_input_shape=[batch_size, None, max_id]),
keras.layers.GRU(128, return_sequences=True, stateful=True,
dropout=0.2, recurrent_dropout=0.2),
keras.layers.TimeDistributed(keras.layers.Dense(max_id,
activation="softmax"))
])

class ResetStatesCallback(keras.callbacks.Callback):
    def on_epoch_begin(self, epoch, logs):
    self.model.reset_states()
    


model.compile(loss="sparse_categorical_crossentropy", optimizer="adam")
model.fit(dataset, epochs=50, callbacks=[ResetStatesCallback()])
#+end_src



* Sentiment Analysis
- IMDb reviews dataset: consists of 50,000 movie reviews in English (25,000 for training, 25,000 for testing) extracted from the famous Internet Movie Database, along with a simple binary target.

- Each review is represented as a NumPy array of integers, where each integer represents a word. 

- All punctuation was removed.

- Words were converted to lowercase and split by spaces

- Indexed by frequency (so low integers correspond to frequent words). 

- The integers 0, 1, and 2 are special
- 1 represents the padding token 
- 2 represents the start-of-sequence (SSS) token
- 3 represents unknown words

- In a real project, preprocess the text yourself, by using the same Tokenizer class as used earlier, wit char_level=False 

- When encoding words, it filters out a lot of characters, including most punctuation, line breaks, and tabs

- Can change this by setting the filters argument. 

- Spaces are used to identify word boundaries. 

- This is OK for English and many other scripts.

- In English, spaces are not always the best way to tokenize
  text: (San Francisco, #ILoveDeepLearning)
- Unsupervised learning technique to tokenize and detokenize
  text at the subword level in a language-independent way
- Treat spaces like other characters
- if your model encounters a word it has never seen before, it can still reasonably guess what it means.

- It may never have seen the word "smartest" during training, but perhaps it learned the word "smart" and it also learned that the suffix “est” means “the most,”

- Other ways of creating subword encodings (e.g., using byte pair encoding).

- Load the original IMDb reviews, as text (byte strings), using TensorFlow Datasets 
- write the preprocessing function


- it truncates the reviews, keeping only the first 300 characters
- it uses regular expressions to replace <br /> tags with spaces, and to replace any characters other than letters and quotes with spaces.
- For example, the text "Well, I can't<br />" will become "Well I can't".
- The preprocess() function splits the reviews by the spaces, which returns a ragged tensor, and it converts this ragged tensor to a dense tensor, padding all reviews with the padding
  token "<pad>" so that they all have the same length.

- Construct the vocabulary:

- This requires going through the whole training set once, applying our preprocess() function, and using a Counter to count the number of occurrences of each word.

- truncate the vocabulary, keeping only the 10,000 most common words:

- Add a preprocessing step to replace each word with its ID
- Create a lookup table for this, using 1,000 out-of-vocabulary (oov) buckets:

- Note that the words “this,” “movie,” and “was” were found in the table, so their IDs are lower than 10,000,

- the word “faaaaaantastic” was not found, so it was mapped to one of the oov buckets, with an ID greater than or equal to 10,000.

- Create the final training set. 

- batch the reviews

- then convert them to short sequences of words using the preprocess() function

- then encode these words using a simple encode_words() function that uses the table we just built,

- prefetch the next batch

- The first layer is an Embedding layer, which will convert word IDs into embeddings. 
- The embedding matrix needs to have one row per word ID (vocab_size + num_oov_buckets) and one column per embedding dimension (this example uses 128 dimensions, but this is a hyperparameter). 

- Inputs of the model will be 2D tensors of shape [batch size, time steps]

- the output of the Embedding layer will be a 3D tensor of shape [batch size, time steps, embedding size].
  


#+begin_src python :result outputs

(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()
X_train[0][:10]
  #[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]


  word_index = keras.datasets.imdb.get_word_index()
  id_to_word = {id_ + 3: word for word, id_ in word_index.items()}
  for id_, token in enumerate(("<pad>", "<sos>", "<unk>")):
      id_to_word[id_] = token
      " ".join([id_to_word[id_] for id_ in X_train[0][:10]])
  #'<sos> this film was just brilliant casting location scenery story'


  import tensorflow_datasets as tfds
  datasets, info = tfds.load("imdb_reviews", as_supervised=True,
  with_info=True)
  train_size = info.splits["train"].num_examples

  def preprocess(X_batch, y_batch):
      X_batch = tf.strings.substr(X_batch, 0, 300)
      X_batch = tf.strings.regex_replace(X_batch, b"<br\\s*/?>", b" ")

      X_batch = tf.strings.regex_replace(X_batch, b"[^a-zA-Z']", b" ")
      X_batch = tf.strings.split(X_batch)
      return X_batch.to_tensor(default_value=b"<pad>"), y_batch
    

  from collections import Counter
  vocabulary = Counter()
  for X_batch, y_batch in datasets["train"].batch(32).map(preprocess):
      for review in X_batch:
          vocabulary.update(list(review.numpy()))
  #Let’s look at the three most common words:
  vocabulary.most_common()[:3]
  #[(b'<pad>', 215797), (b'the', 61137), (b'a', 38564)]


  vocab_size = 10000
  truncated_vocabulary = [
  word for word, count in vocabulary.most_common()[:vocab_size]
  ]

  words = tf.constant(truncated_vocabulary)
  word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)
  vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)
  num_oov_buckets = 1000
  table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)
  #We can then use this table to look up the IDs of a few words:
  table.lookup(tf.constant([b"This movie was faaaaaantastic".split()]))
  # <tf.Tensor: [...], dtype=int64, numpy=array([[
  # 22,
  # 12,
  # 11,
  # 10054]])>

def encode_words(X_batch, y_batch):
    return table.lookup(X_batch), y_batch
train_set = datasets["train"].batch(32).map(preprocess)
train_set = train_set.map(encode_words).prefetch(1)
#At last we can create the model and train it:
embed_size = 128
model = keras.models.Sequential([
keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,
input_shape=[None]),
keras.layers.GRU(128, return_sequences=True),
keras.layers.GRU(128),
keras.layers.Dense(1, activation="sigmoid")
])
model.compile(loss="binary_crossentropy", optimizer="adam",
metrics=["accuracy"])
history = model.fit(train_set, epochs=5)
#+end_src

* Masking

- the model will need to learn that the padding tokens should be ignored.
- simply add mask_zero=True when creating the Embedding layer
- This means that padding tokens (whose ID is 0) will be ignored by all downstream layers.
- The way this works is that the Embedding layer creates a mask tensor equal to K.not_equal(inputs, 0) (where K = keras.backend): it is a Boolean tensor with the same shape as the inputs, and it is equal to False anywhere the word IDs are 0, or True otherwise.

- This mask tensor is then automatically propagated by the model to all subsequent layers, as long as the time dimension is preserved.

- So in this example, both GRU layers will receive this mask automatically, but since the second GRU layer does not return sequences (it only returns the output of the last time step), the mask will not be transmitted to the Dense layer.

- Each layer may handle the mask differently, but in general they simply ignore masked time steps (i.e., time steps for which the mask is False).

- For example, when a recurrent layer encounters a masked time step, it simply copies the output from the previous time step.

- If the mask propagates all the way to the output (in models that output sequences, which is not the case in this example), then it will be applied to the losses as well, so the masked time steps will not contribute to the loss (their loss will be 0).

- The LSTM and GRU layers have an optimized implementation for GPUs, based on Nvidia’s cuDNN library. However, this implementation does not support masking. If your model uses a mask, then these layers will fall back to the (much slower) default
implementation. Note that the optimized implementation also requires you to use the
default values for several hyperparameters: activation , recurrent_activation ,
recurrent_dropout , unroll , use_bias , and reset_after .

All layers that receive the mask must support masking (or else an
exception will be raised). This includes all recurrent layers, as well as the
TimeDistributed layer and a few other layers. Any layer that supports
masking must have a supports_masking attribute equal to True. If you
want to implement your own custom layer with masking support, you
should add a mask argument to the call() method (and obviously make
the method use the mask somehow). Additionally, you should set
self.supports_masking = True in the constructor. If your layer does
not start with an Embedding layer, you may use the
keras.layers.Masking layer instead: it sets the mask to
K.any(K.not_equal(inputs, 0), axis=-1), meaning that time steps
where the last dimension is full of zeros will be masked out in subsequent
layers (again, as long as the time dimension exists).
Using masking layers and automatic mask propagation works best for
simple Sequential models. It will not always work for more complex
models, such as when you need to mix Conv1D layers with recurrent layers.
In such cases, you will need to explicitly compute the mask and pass it to
the appropriate layers, using either the Functional API or the Subclassing
API. For example, the following model is identical to the previous model,
except it is built using the Functional API and handles masking manually:
K = keras.backend
inputs = keras.layers.Input(shape=[None])
mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)
z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)
(inputs)
z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)
z = keras.layers.GRU(128)(z, mask=mask)
outputs = keras.layers.Dense(1, activation="sigmoid")(z)
model = keras.Model(inputs=[inputs], outputs=[outputs])
After training for a few epochs, this model will become quite good at
judging whether a review is positive or not. If you use the TensorBoard()
callback, you can visualize the embeddings in TensorBoard as they are
being learned: it is fascinating to see words like “awesome” and


“amazing” gradually cluster on one side of the embedding space, while
words like “awful” and “terrible” cluster on the other side. Some words
are not as positive as you might expect (at least with this model), such as
the word “good,” presumably because many negative reviews contain the
phrase “not good.” It’s impressive that the model is able to learn useful
word embeddings based on just 25,000 movie reviews. Imagine how good
the embeddings would be if we had billions of reviews to train on!
Unfortunately we don’t, but perhaps we can reuse word embeddings
trained on some other large text corpus (e.g., Wikipedia articles), even if it
is not composed of movie reviews? After all, the word “amazing”
generally has the same meaning whether you use it to talk about movies or
anything else. Moreover, perhaps embeddings would be useful for
sentiment analysis even if they were trained on another task: since words
like “awesome” and “amazing” have a similar meaning, they will likely
cluster in the embedding space even for other tasks (e.g., predicting the
next word in a sentence). If all positive words and all negative words form
clusters, then this will be helpful for sentiment analysis. So instead of
using so many parameters to learn word embeddings, let’s see if we can’t
just reuse pretrained embeddings.

