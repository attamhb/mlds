* Machine Learning Basics
Machine learning:
form of applied statistics,
statistically estimate complicated functions
a decreased emphasis on proving conﬁdence intervals 
Frequentist estimators
Bayesian inference 
** Learning Algorithms
Able to learn from data. 
*Learning*: A program learns from experience E with respect to some task T and performance measure P, if P in T improves with E
*** The Task, T
Understanding of machine learning entails developing our understanding of the principles that underlie intelligence

Learning is our means of attaining the ability to perform the task

*** Classiﬁcation:
The computer program is asked to specify which of k categories some input belongs to. 
 y=f: R^n -\to {1,...,k}

Probability distribution over classes: 
Object recognition
Input is an image and the output is a numeric code identifying the object in the image
 
*** Classiﬁcation with missing inputs:
The learning algorithm only has to deﬁne a single function mapping from a vector input to a categorical output

When some of the inputs may be missing,
rather than providing a single classiﬁcation function, the learning algorithm must learn a set of functions.

Each function corresponds to classifying x with a diﬀerent subset of its inputs missing.

One way to eﬃciently deﬁne such a large set of functions is 
to learn a probability distribution over all of the relevant 
variables, then solve the classiﬁcation task by marginalizing 
out the missing variables.

With n input variables, we can now 
obtain all 2^n diﬀerent classiﬁcation functions needed for
each possible set of missing inputs, but we only need to learn 
a single function describing the joint probability 
distribution. 

*** Regression:
The computer program is asked to predict a
numerical value given some input  f:R^n → R

*** Transcription:
The machine learning system is asked to observe a relatively 
unstructured representation of some kind of data and 
transcribe it into discrete, textual form.  

Examples: Google Street View, Speech recognition 

*** Machine translation:
The input already consists of a sequence of symbols in some 
language, and the computer program must convert this into a 
sequence of symbols in another language. 

*** Structured output:
Such tasks involve any task where the
output is a vector with important relationships between the diﬀerent elements

Example: Parsing—mapping a natural language sentence into a 
tree that describes its grammatical structure and tagging 
nodes of the trees as being verbs, nouns, or adverbs, and so 
on. 

*** Anomaly detection:
The computer program ﬂags some of them as being anomaly

*** Synthesis and sampling:
The machine learning algorithm is asked to generate new examples that are similar to those in the training data

*** Imputation of missing values:
The learning algorithm is given a new example x ∈ R^n, 
but with some entries x_i of x missing. The algorithm must provide a prediction of the values of the missing entries.
*** Denoising:
The algorithm is given in input a corrupted example x̃ ∈ R^n 
obtained by an unknown corruption process from a clean example 
x ∈ R^n.

The learner must predict the clean example x from its
corrupted version x̃, or more generally predict the conditional 
probability distribution p(x|x̃).

*** Density estimation or probability mass function estimation:
The algorithm is asked to learn a function p_model : R^n → R. 
** The Performance Measure, P
For classiﬁcation: Measure the accuracy 
Error rate:  The expected 0-1 loss
For density estimation: The average log-probability 

** The Experience, E
*Unsupervised Learning Algorithms:*
In the context of deep learning, we usually want to learn the entire probability distribution that generated a dataset

*Supervised Learning Algorithms:* It involves observing several 
examples of a random vector x and an associated value or vector 
y, and learning to predict y from x, usually by estimating 
p(y|x).


The lines between them are often blurred. Many machine learning 
technologies can be used to perform both tasks. For example, the 
chain rule of probability states that for a vector x ∈ R^n, the 
joint distribution can be decomposed as
 
           p(x) = \prod_{i=1:n} p(x_i|x_1, x_2, ..., x_n)

           
We can solve unsupervised problem of modeling p(x) by splitting 
it into n supervised learning problems.

We can solve the supervised learning problem of learning 
p(y|x) by using traditional unsupervised learning technologies 
to learn the joint distribution p(x,y) and inferring

             p(y|x) = p(x|y)/\sum_{y'}p(x,y')
             

Traditionally, people refer to regression, classiﬁcation and structured output problems as supervised learning.

Density estimation in support of other tasks is usually considered unsupervised learning.

In semi- supervised learning, some examples include a supervision target but others do not.

In multi-instance learning, an entire collection of examples is 
labeled as containing or not containing an example of a class, 
but the individual members of the collection are not labeled. 

Some machine learning algorithms do not just experience a ﬁxed 
dataset. For example, reinforcement learning. 

** Example: Linear Regression
Build a system that can take a vector x \in R^n as input and 
predict the value of a scalar y \in R as its output. The output is 
a linear function of the input. 

Let \hat{y} the value that our model predicts y should take on. 
We deﬁne the output to be 
\hat{y} = w^T \cdot x where w \in R^n is a vector of parameters. 

We thus have a deﬁnition of our task T : to predict y from x by outputting ŷ = w^T \cdot x.

** Capacity, Overﬁtting and Underﬁtting

*Generalization* The ability to perform well on previously unobserved inputs is called generalization.

*Generalization error* is deﬁned as the expected value of the 
error on a new input. (taken across 
diﬀerent possible inputs)  

*Data generating process*
The train and test data are generated by a probability 
distribution over datasets called the data generating process. 


*Assumptions*: The examples in each dataset are independent from 
each other, and that the train set and test set are identically 
distributed, drawn from the same probability distribution as 
each other. 

*Data Generating Distribution*
The same distribution is then used to generate every train example and every test example,  denoted p_data . 


The expected training error of a randomly selected model is equal to the expected test error of that model.

We sample the training set, then use it to choose the parameters to reduce training set error, then sample the test set.

The factors determining how well a machine learning algorithm will perform are its ability to:

1. Make the training error small. 
2. Make the gap between training and test error small.

*Capacity* A model's capacity is its ability to ﬁt a wide variety 
of functions. 

*Underfitting:* Models with low capacity may struggle to ﬁt the 
training set. 

*Overfitting:*
Models with high capacity can overﬁt by memorizing properties of 
the training set that do not serve them well on the test set. 

*A Solution*
One way to control the capacity of a learning algorithm is by 
choosing its hypothesis space, the set of functions that the 
learning algorithm is allowed to select as being the solution. 

Including polynomials instead of linear functions in the regression algorithm. 


Machine learning algorithms will generally perform best when 
their capacity is appropriate for the true complexity of the 
task they need to perform and the amount of training data they 
are provided with. 


*Representational Capacity*
The model speciﬁes which family of functions the learning algorithm can choose from when varying the parameters in order to reduce a training objective. 

In many cases, ﬁnding the best function within this family is a very diﬃcult optimization problem.

These additional limitations, such as the imperfection of the 
optimization algorithm, mean that the learning algorithm’s 
eﬀective capacity may be less than the representational capacity 
of the model family.

*Vapnik-Chervonenkis dimension* The VC dimension measures the 
capacity of a binary classiﬁer and is deﬁned as being the 
largest possible value of m for which there exists a training 
set of m diﬀerent x points that the classiﬁer can label 
arbitrarily.

Quantifying the capacity of the model allows statistical learning theory to make quantitative predictions

The discrepancy between training error and generalization error 
is bounded from above by a quantity that grows as the model 
capacity grows but shrinks as the number of training examples 
increase.

Typically, training error decreases until it asymptotes to the 
minimum possible error value as model capacity increases. 

Typically, generalization error has a U-shaped curve as a 
function of model capacity.

*Non-parameteric Models*
Nearest neighbor regression model simply stores the X and y from 
the training set. When asked to classify a test point x, the 
model looks up the nearest entry in the training set and returns 
the associated regression target. 

We can also create a non-parametric learning algorithm by 
wrapping a parametric learning algorithm inside another 
algorithm that increases the number of parameters as needed.

**  The No Free Lunch Theorem
Learning theory claims that a machine learning algorithm can 
generalize well from a ﬁnite training set of examples.

This seems to contradict some basic principles of logic. 

Inductive reasoning, or inferring general rules from a limited 
set of examples, is not logically valid. 

To logically infer a rule describing every member of a set, one must have information about every member of that set. 

In part, machine learning avoids this problem by oﬀering only 
probabilistic rules, rather than the entirely certain rules used 
in purely logical reasoning. 

The no free lunch theorem for machine learning states that, averaged over all possible data generating distributions, every classiﬁcation algorithm has the same error rate when classifying previously unobserved points.

No machine learning algorithm is universally any better than any other.

The goal of machine learning research is not to seek a universal 
learning algorithm or the absolute best learning algorithm. 

** Regularization
The no free lunch theorem implies that we must design our 
machine learning algorithms to perform well on a speciﬁc task. 

We do so by building a set of preferences into the learning 
algorithm. When these preferences are aligned with the learning 
problems we ask the algorithm to solve, it performs better.

The behavior of our algorithm is strongly aﬀected not just by how large we make the set of functions allowed in its hypothesis space, but by the speciﬁc identity of those functions. 

We can also give a learning algorithm a preference for one 
solution in its hypothesis space to another. The unpreferred 
solution will be chosen only if it ﬁts the training


We can modify the training criterion for linear regression to 
include weight decay. 


We can regularize a model that learns a function f(x; \theta) by 
adding a penalty called a regularizer to the cost function.

** Hyperparameters and Validation Sets
Most machine learning algorithms have several settings that we 
can use to control the behavior of the learning algorithm. These 
settings are called hyperparameters.

More frequently, the setting must be a hyperparameter because it is not appropriate to learn that hyperparameter on the training set.

Split the training data into two disjoint subsets. One of these 
subsets is used to learn the parameters. The other subset is our 
validation set, used to estimate the generalization error during 
or after training, allowing for the hyperparameters to be 
updated accordingly. 

The subset of data used to learn the parameters is still 
typically called the training set, even though this may be 
confused with the larger pool of data used for the entire 
training process.

The subset of data used to guide the selection of 
hyperparameters is called the validation set.


The validation set error will underestimate the generalization 
error, though typically by a smaller amount than the training 
error.

After all hyperparameter optimization is complete, the 
generalization error may be estimated using the test set.

** Cross-Validation
*k-fold cross-validation* A partition of the dataset is formed by 
splitting it into k non-overlapping subsets. The test error may 
then be estimated by taking the average test error across k 
trials. One problem is that there exist no unbiased estimators 
of the variance of such average error estimators, but 
approximations are typically used.

**  Estimators, Bias and Variance 
The ﬁeld of statistics gives us many tools that can be used to achieve the
machine learning goal of solving a task not only on the training set but also to
generalize. Foundational concepts such as parameter estimation, bias and
variance are useful to formally characterize notions of generalization,
underﬁtting and overﬁtting. 5.4.1 Point Estimation Point estimation is the
attempt to provide the single “best” prediction of some quantity of interest. In
general the quantity of interest can be a single parameter or a vector of
parameters in some parametric model, such as the weights in our linear
regression example in section 5.1.4, but it can also be a whole function. In
order to distinguish estimates of parameters from their true value, our
convention will be to denote a point estimate of a parameter θ by θ̂. Let {x(1) ,
. . . , x(m) } be a set of m independent and identically distributed
